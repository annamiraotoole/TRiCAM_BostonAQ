{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import statsmodels.api as sm\n",
    "sns.set(style=\"ticks\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ucidata.csv', sep=\",\", header=None)\n",
    "\n",
    "#replaces question marks with \"0\"\n",
    "for c in range(0, 127):\n",
    "    for r in range(0, 1994):\n",
    "        if df[c][r] == \"?\":\n",
    "            df.set_value(r, c, 0)\n",
    "            \n",
    "#turns all columns that are numerics encoded as strings into floats\n",
    "for c in range(4, 127):\n",
    "    for r in range(0, 1994):\n",
    "        if type(df[c][r]) == str:\n",
    "            df.set_value(r, c, float(df[c][r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes out features columns\n",
    "x = df.iloc[:, 4:126]\n",
    "\n",
    "#takes of what we want to predict\n",
    "target = df.iloc[:, 127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>127</td>       <th>  R-squared:         </th> <td>   0.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   88.95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 30 Jun 2017</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:41:05</td>     <th>  Log-Likelihood:    </th> <td>  1273.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1994</td>      <th>  AIC:               </th> <td>  -2304.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1872</td>      <th>  BIC:               </th> <td>  -1621.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   122</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>   -0.0013</td> <td>    0.001</td> <td>   -1.254</td> <td> 0.210</td> <td>   -0.003     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.1734</td> <td>    0.402</td> <td>    0.432</td> <td> 0.666</td> <td>   -0.614     0.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>   -0.0173</td> <td>    0.087</td> <td>   -0.200</td> <td> 0.842</td> <td>   -0.187     0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.2056</td> <td>    0.051</td> <td>    4.053</td> <td> 0.000</td> <td>    0.106     0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>   -0.0236</td> <td>    0.058</td> <td>   -0.411</td> <td> 0.681</td> <td>   -0.136     0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0007</td> <td>    0.034</td> <td>    0.022</td> <td> 0.983</td> <td>   -0.065     0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0933</td> <td>    0.054</td> <td>    1.741</td> <td> 0.082</td> <td>   -0.012     0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>    0.1785</td> <td>    0.104</td> <td>    1.723</td> <td> 0.085</td> <td>   -0.025     0.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>  <td>   -0.0861</td> <td>    0.150</td> <td>   -0.575</td> <td> 0.566</td> <td>   -0.380     0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>  <td>   -0.2625</td> <td>    0.160</td> <td>   -1.640</td> <td> 0.101</td> <td>   -0.576     0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th>  <td>    0.1597</td> <td>    0.096</td> <td>    1.659</td> <td> 0.097</td> <td>   -0.029     0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>   -0.3147</td> <td>    0.390</td> <td>   -0.807</td> <td> 0.420</td> <td>   -1.080     0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>  <td>    0.0509</td> <td>    0.016</td> <td>    3.222</td> <td> 0.001</td> <td>    0.020     0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>  <td>   -0.1431</td> <td>    0.173</td> <td>   -0.825</td> <td> 0.409</td> <td>   -0.483     0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>   -0.1212</td> <td>    0.086</td> <td>   -1.414</td> <td> 0.158</td> <td>   -0.289     0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>  <td>    0.0461</td> <td>    0.020</td> <td>    2.286</td> <td> 0.022</td> <td>    0.007     0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.1419</td> <td>    0.066</td> <td>   -2.149</td> <td> 0.032</td> <td>   -0.271    -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>  <td>    0.1356</td> <td>    0.107</td> <td>    1.270</td> <td> 0.204</td> <td>   -0.074     0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>  <td>    0.0254</td> <td>    0.046</td> <td>    0.554</td> <td> 0.580</td> <td>   -0.065     0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>  <td>   -0.0751</td> <td>    0.036</td> <td>   -2.066</td> <td> 0.039</td> <td>   -0.146    -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.2560</td> <td>    0.161</td> <td>    1.591</td> <td> 0.112</td> <td>   -0.060     0.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>    0.1094</td> <td>    0.192</td> <td>    0.569</td> <td> 0.570</td> <td>   -0.268     0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.3420</td> <td>    0.156</td> <td>   -2.191</td> <td> 0.029</td> <td>   -0.648    -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>   -0.0335</td> <td>    0.026</td> <td>   -1.313</td> <td> 0.189</td> <td>   -0.084     0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.0333</td> <td>    0.019</td> <td>   -1.712</td> <td> 0.087</td> <td>   -0.071     0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.0230</td> <td>    0.019</td> <td>    1.212</td> <td> 0.225</td> <td>   -0.014     0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>  <td>    0.0447</td> <td>    0.019</td> <td>    2.387</td> <td> 0.017</td> <td>    0.008     0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>    0.0315</td> <td>    0.025</td> <td>    1.265</td> <td> 0.206</td> <td>   -0.017     0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.0928</td> <td>    0.160</td> <td>    0.578</td> <td> 0.563</td> <td>   -0.222     0.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>  <td>   -0.1341</td> <td>    0.061</td> <td>   -2.187</td> <td> 0.029</td> <td>   -0.254    -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.1090</td> <td>    0.068</td> <td>   -1.601</td> <td> 0.110</td> <td>   -0.242     0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>    0.0829</td> <td>    0.096</td> <td>    0.862</td> <td> 0.389</td> <td>   -0.106     0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>    0.0717</td> <td>    0.078</td> <td>    0.923</td> <td> 0.356</td> <td>   -0.081     0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>  <td>    0.0253</td> <td>    0.041</td> <td>    0.622</td> <td> 0.534</td> <td>   -0.054     0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.2940</td> <td>    0.078</td> <td>    3.771</td> <td> 0.000</td> <td>    0.141     0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>   -0.0708</td> <td>    0.032</td> <td>   -2.207</td> <td> 0.027</td> <td>   -0.134    -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>  <td>   -0.0205</td> <td>    0.041</td> <td>   -0.499</td> <td> 0.618</td> <td>   -0.101     0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>    0.0772</td> <td>    0.055</td> <td>    1.402</td> <td> 0.161</td> <td>   -0.031     0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>    0.1262</td> <td>    0.086</td> <td>    1.461</td> <td> 0.144</td> <td>   -0.043     0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>43</th>  <td>    0.5028</td> <td>    0.248</td> <td>    2.031</td> <td> 0.042</td> <td>    0.017     0.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>  <td>    0.2503</td> <td>    0.068</td> <td>    3.687</td> <td> 0.000</td> <td>    0.117     0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>45</th>  <td>    0.2068</td> <td>    0.309</td> <td>    0.668</td> <td> 0.504</td> <td>   -0.400     0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>   -0.5808</td> <td>    0.520</td> <td>   -1.117</td> <td> 0.264</td> <td>   -1.600     0.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.1260</td> <td>    0.170</td> <td>   -0.741</td> <td> 0.459</td> <td>   -0.459     0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>  <td>    0.0479</td> <td>    0.161</td> <td>    0.298</td> <td> 0.766</td> <td>   -0.268     0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>   -0.2664</td> <td>    0.155</td> <td>   -1.724</td> <td> 0.085</td> <td>   -0.570     0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>   -0.0294</td> <td>    0.048</td> <td>   -0.608</td> <td> 0.543</td> <td>   -0.124     0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>   -0.0015</td> <td>    0.043</td> <td>   -0.035</td> <td> 0.972</td> <td>   -0.085     0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.0651</td> <td>    0.047</td> <td>    1.383</td> <td> 0.167</td> <td>   -0.027     0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>53</th>  <td>   -0.1907</td> <td>    0.054</td> <td>   -3.537</td> <td> 0.000</td> <td>   -0.296    -0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.1727</td> <td>    0.138</td> <td>   -1.250</td> <td> 0.211</td> <td>   -0.444     0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>    0.1450</td> <td>    0.047</td> <td>    3.092</td> <td> 0.002</td> <td>    0.053     0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>56</th>  <td>   -0.1811</td> <td>    0.094</td> <td>   -1.937</td> <td> 0.053</td> <td>   -0.364     0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>    0.0257</td> <td>    0.041</td> <td>    0.626</td> <td> 0.531</td> <td>   -0.055     0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>  <td>    0.0293</td> <td>    0.066</td> <td>    0.441</td> <td> 0.659</td> <td>   -0.101     0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>59</th>  <td>   -0.0901</td> <td>    0.077</td> <td>   -1.169</td> <td> 0.243</td> <td>   -0.241     0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0540</td> <td>    0.060</td> <td>    0.905</td> <td> 0.366</td> <td>   -0.063     0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.0456</td> <td>    0.123</td> <td>   -0.371</td> <td> 0.710</td> <td>   -0.286     0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1849</td> <td>    0.222</td> <td>   -0.834</td> <td> 0.404</td> <td>   -0.620     0.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.4630</td> <td>    0.273</td> <td>    1.693</td> <td> 0.091</td> <td>   -0.073     0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.2483</td> <td>    0.220</td> <td>   -1.131</td> <td> 0.258</td> <td>   -0.679     0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>    0.0282</td> <td>    0.067</td> <td>    0.422</td> <td> 0.673</td> <td>   -0.103     0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>  <td>   -0.1533</td> <td>    0.069</td> <td>   -2.217</td> <td> 0.027</td> <td>   -0.289    -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>  <td>   -0.0056</td> <td>    0.230</td> <td>   -0.025</td> <td> 0.980</td> <td>   -0.456     0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>  <td>   -0.2018</td> <td>    0.238</td> <td>   -0.847</td> <td> 0.397</td> <td>   -0.669     0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>69</th>  <td>    0.6435</td> <td>    0.252</td> <td>    2.556</td> <td> 0.011</td> <td>    0.150     1.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.0409</td> <td>    0.169</td> <td>   -0.242</td> <td> 0.809</td> <td>   -0.372     0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>   -0.1997</td> <td>    0.080</td> <td>   -2.494</td> <td> 0.013</td> <td>   -0.357    -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>  <td>   -0.6307</td> <td>    0.362</td> <td>   -1.742</td> <td> 0.082</td> <td>   -1.341     0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>73</th>  <td>    0.1813</td> <td>    0.077</td> <td>    2.352</td> <td> 0.019</td> <td>    0.030     0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>    0.1536</td> <td>    0.056</td> <td>    2.728</td> <td> 0.006</td> <td>    0.043     0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>75</th>  <td>    0.0381</td> <td>    0.019</td> <td>    1.967</td> <td> 0.049</td> <td>    0.000     0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>    0.1670</td> <td>    0.076</td> <td>    2.208</td> <td> 0.027</td> <td>    0.019     0.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>   -0.0435</td> <td>    0.031</td> <td>   -1.397</td> <td> 0.163</td> <td>   -0.105     0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>78</th>  <td>    0.5242</td> <td>    0.379</td> <td>    1.383</td> <td> 0.167</td> <td>   -0.219     1.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>    0.0420</td> <td>    0.022</td> <td>    1.936</td> <td> 0.053</td> <td>   -0.001     0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.0668</td> <td>    0.025</td> <td>   -2.679</td> <td> 0.007</td> <td>   -0.116    -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>   -0.0072</td> <td>    0.029</td> <td>   -0.249</td> <td> 0.803</td> <td>   -0.064     0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0162</td> <td>    0.035</td> <td>    0.457</td> <td> 0.647</td> <td>   -0.053     0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>   -0.0154</td> <td>    0.020</td> <td>   -0.757</td> <td> 0.449</td> <td>   -0.055     0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>   -0.3838</td> <td>    0.203</td> <td>   -1.888</td> <td> 0.059</td> <td>   -0.782     0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>    0.2935</td> <td>    0.308</td> <td>    0.953</td> <td> 0.341</td> <td>   -0.311     0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>   -0.0045</td> <td>    0.167</td> <td>   -0.027</td> <td> 0.978</td> <td>   -0.331     0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>   -0.2451</td> <td>    0.067</td> <td>   -3.673</td> <td> 0.000</td> <td>   -0.376    -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0313</td> <td>    0.157</td> <td>   -0.199</td> <td> 0.842</td> <td>   -0.339     0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0511</td> <td>    0.086</td> <td>   -0.591</td> <td> 0.555</td> <td>   -0.221     0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>90</th>  <td>    0.3623</td> <td>    0.130</td> <td>    2.786</td> <td> 0.005</td> <td>    0.107     0.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>  <td>    0.0553</td> <td>    0.033</td> <td>    1.701</td> <td> 0.089</td> <td>   -0.008     0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.0377</td> <td>    0.034</td> <td>   -1.093</td> <td> 0.275</td> <td>   -0.105     0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>   -0.0856</td> <td>    0.025</td> <td>   -3.482</td> <td> 0.001</td> <td>   -0.134    -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>  <td>    0.1333</td> <td>    0.069</td> <td>    1.944</td> <td> 0.052</td> <td>   -0.001     0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>    0.1585</td> <td>    0.049</td> <td>    3.220</td> <td> 0.001</td> <td>    0.062     0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>96</th>  <td>    0.1732</td> <td>    0.090</td> <td>    1.922</td> <td> 0.055</td> <td>   -0.004     0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>    0.0265</td> <td>    0.041</td> <td>    0.642</td> <td> 0.521</td> <td>   -0.054     0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>   -0.0155</td> <td>    0.058</td> <td>   -0.268</td> <td> 0.789</td> <td>   -0.129     0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0314</td> <td>    0.038</td> <td>    0.823</td> <td> 0.411</td> <td>   -0.043     0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>    0.0091</td> <td>    0.043</td> <td>    0.214</td> <td> 0.831</td> <td>   -0.075     0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>    0.0574</td> <td>    0.343</td> <td>    0.167</td> <td> 0.867</td> <td>   -0.615     0.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>  -22.7480</td> <td>   14.173</td> <td>   -1.605</td> <td> 0.109</td> <td>  -50.545     5.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.0751</td> <td>    0.083</td> <td>    0.902</td> <td> 0.367</td> <td>   -0.088     0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.5146</td> <td>    0.371</td> <td>    1.386</td> <td> 0.166</td> <td>   -0.213     1.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>105</th> <td>   -0.1920</td> <td>    0.170</td> <td>   -1.130</td> <td> 0.259</td> <td>   -0.525     0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>106</th> <td>   -0.0618</td> <td>    0.145</td> <td>   -0.427</td> <td> 0.669</td> <td>   -0.345     0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.1832</td> <td>    0.087</td> <td>    2.116</td> <td> 0.034</td> <td>    0.013     0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>108</th> <td>   22.3362</td> <td>   14.149</td> <td>    1.579</td> <td> 0.115</td> <td>   -5.413    50.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>109</th> <td>   -0.0894</td> <td>    0.045</td> <td>   -1.984</td> <td> 0.047</td> <td>   -0.178    -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>110</th> <td>   -0.0085</td> <td>    0.082</td> <td>   -0.105</td> <td> 0.917</td> <td>   -0.168     0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>    0.0345</td> <td>    0.144</td> <td>    0.240</td> <td> 0.811</td> <td>   -0.248     0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0549</td> <td>    0.154</td> <td>    0.357</td> <td> 0.721</td> <td>   -0.247     0.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>113</th> <td>    0.0626</td> <td>    0.047</td> <td>    1.339</td> <td> 0.181</td> <td>   -0.029     0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114</th> <td>   -0.0217</td> <td>    0.203</td> <td>   -0.107</td> <td> 0.915</td> <td>   -0.420     0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>115</th> <td>    0.0123</td> <td>    0.207</td> <td>    0.060</td> <td> 0.952</td> <td>   -0.393     0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>116</th> <td>   -0.0146</td> <td>    0.042</td> <td>   -0.348</td> <td> 0.728</td> <td>   -0.097     0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>117</th> <td>   -0.0217</td> <td>    0.036</td> <td>   -0.601</td> <td> 0.548</td> <td>   -0.092     0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>118</th> <td>    0.0256</td> <td>    0.051</td> <td>    0.504</td> <td> 0.614</td> <td>   -0.074     0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>119</th> <td>    0.0044</td> <td>    0.031</td> <td>    0.144</td> <td> 0.886</td> <td>   -0.056     0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>120</th> <td>   -0.0441</td> <td>    0.023</td> <td>   -1.903</td> <td> 0.057</td> <td>   -0.090     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>121</th> <td>    0.0940</td> <td>    0.081</td> <td>    1.162</td> <td> 0.245</td> <td>   -0.065     0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>122</th> <td>    0.1787</td> <td>    0.272</td> <td>    0.656</td> <td> 0.512</td> <td>   -0.355     0.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>123</th> <td>   -0.0648</td> <td>    0.067</td> <td>   -0.972</td> <td> 0.331</td> <td>   -0.196     0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>124</th> <td>    0.0302</td> <td>    0.020</td> <td>    1.518</td> <td> 0.129</td> <td>   -0.009     0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>125</th> <td>   -0.0511</td> <td>    0.039</td> <td>   -1.319</td> <td> 0.187</td> <td>   -0.127     0.025</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>389.508</td> <th>  Durbin-Watson:     </th> <td>   1.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1263.113</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.967</td>  <th>  Prob(JB):          </th> <td>5.23e-275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.385</td>  <th>  Cond. No.          </th> <td>4.91e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    127   R-squared:                       0.853\n",
       "Model:                            OLS   Adj. R-squared:                  0.843\n",
       "Method:                 Least Squares   F-statistic:                     88.95\n",
       "Date:                Fri, 30 Jun 2017   Prob (F-statistic):               0.00\n",
       "Time:                        10:41:05   Log-Likelihood:                 1273.9\n",
       "No. Observations:                1994   AIC:                            -2304.\n",
       "Df Residuals:                    1872   BIC:                            -1621.\n",
       "Df Model:                         122                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "4             -0.0013      0.001     -1.254      0.210        -0.003     0.001\n",
       "5              0.1734      0.402      0.432      0.666        -0.614     0.961\n",
       "6             -0.0173      0.087     -0.200      0.842        -0.187     0.153\n",
       "7              0.2056      0.051      4.053      0.000         0.106     0.305\n",
       "8             -0.0236      0.058     -0.411      0.681        -0.136     0.089\n",
       "9              0.0007      0.034      0.022      0.983        -0.065     0.067\n",
       "10             0.0933      0.054      1.741      0.082        -0.012     0.198\n",
       "11             0.1785      0.104      1.723      0.085        -0.025     0.382\n",
       "12            -0.0861      0.150     -0.575      0.566        -0.380     0.208\n",
       "13            -0.2625      0.160     -1.640      0.101        -0.576     0.051\n",
       "14             0.1597      0.096      1.659      0.097        -0.029     0.349\n",
       "15            -0.3147      0.390     -0.807      0.420        -1.080     0.451\n",
       "16             0.0509      0.016      3.222      0.001         0.020     0.082\n",
       "17            -0.1431      0.173     -0.825      0.409        -0.483     0.197\n",
       "18            -0.1212      0.086     -1.414      0.158        -0.289     0.047\n",
       "19             0.0461      0.020      2.286      0.022         0.007     0.086\n",
       "20            -0.1419      0.066     -2.149      0.032        -0.271    -0.012\n",
       "21             0.1356      0.107      1.270      0.204        -0.074     0.345\n",
       "22             0.0254      0.046      0.554      0.580        -0.065     0.115\n",
       "23            -0.0751      0.036     -2.066      0.039        -0.146    -0.004\n",
       "24             0.2560      0.161      1.591      0.112        -0.060     0.572\n",
       "25             0.1094      0.192      0.569      0.570        -0.268     0.486\n",
       "26            -0.3420      0.156     -2.191      0.029        -0.648    -0.036\n",
       "27            -0.0335      0.026     -1.313      0.189        -0.084     0.017\n",
       "28            -0.0333      0.019     -1.712      0.087        -0.071     0.005\n",
       "29             0.0230      0.019      1.212      0.225        -0.014     0.060\n",
       "30             0.0447      0.019      2.387      0.017         0.008     0.081\n",
       "31             0.0315      0.025      1.265      0.206        -0.017     0.080\n",
       "32             0.0928      0.160      0.578      0.563        -0.222     0.408\n",
       "33            -0.1341      0.061     -2.187      0.029        -0.254    -0.014\n",
       "34            -0.1090      0.068     -1.601      0.110        -0.242     0.025\n",
       "35             0.0829      0.096      0.862      0.389        -0.106     0.271\n",
       "36             0.0717      0.078      0.923      0.356        -0.081     0.224\n",
       "37             0.0253      0.041      0.622      0.534        -0.054     0.105\n",
       "38             0.2940      0.078      3.771      0.000         0.141     0.447\n",
       "39            -0.0708      0.032     -2.207      0.027        -0.134    -0.008\n",
       "40            -0.0205      0.041     -0.499      0.618        -0.101     0.060\n",
       "41             0.0772      0.055      1.402      0.161        -0.031     0.185\n",
       "42             0.1262      0.086      1.461      0.144        -0.043     0.296\n",
       "43             0.5028      0.248      2.031      0.042         0.017     0.988\n",
       "44             0.2503      0.068      3.687      0.000         0.117     0.383\n",
       "45             0.2068      0.309      0.668      0.504        -0.400     0.814\n",
       "46            -0.5808      0.520     -1.117      0.264        -1.600     0.439\n",
       "47            -0.1260      0.170     -0.741      0.459        -0.459     0.207\n",
       "48             0.0479      0.161      0.298      0.766        -0.268     0.364\n",
       "49            -0.2664      0.155     -1.724      0.085        -0.570     0.037\n",
       "50            -0.0294      0.048     -0.608      0.543        -0.124     0.065\n",
       "51            -0.0015      0.043     -0.035      0.972        -0.085     0.082\n",
       "52             0.0651      0.047      1.383      0.167        -0.027     0.157\n",
       "53            -0.1907      0.054     -3.537      0.000        -0.296    -0.085\n",
       "54            -0.1727      0.138     -1.250      0.211        -0.444     0.098\n",
       "55             0.1450      0.047      3.092      0.002         0.053     0.237\n",
       "56            -0.1811      0.094     -1.937      0.053        -0.364     0.002\n",
       "57             0.0257      0.041      0.626      0.531        -0.055     0.106\n",
       "58             0.0293      0.066      0.441      0.659        -0.101     0.160\n",
       "59            -0.0901      0.077     -1.169      0.243        -0.241     0.061\n",
       "60             0.0540      0.060      0.905      0.366        -0.063     0.171\n",
       "61            -0.0456      0.123     -0.371      0.710        -0.286     0.195\n",
       "62            -0.1849      0.222     -0.834      0.404        -0.620     0.250\n",
       "63             0.4630      0.273      1.693      0.091        -0.073     0.999\n",
       "64            -0.2483      0.220     -1.131      0.258        -0.679     0.182\n",
       "65             0.0282      0.067      0.422      0.673        -0.103     0.159\n",
       "66            -0.1533      0.069     -2.217      0.027        -0.289    -0.018\n",
       "67            -0.0056      0.230     -0.025      0.980        -0.456     0.444\n",
       "68            -0.2018      0.238     -0.847      0.397        -0.669     0.266\n",
       "69             0.6435      0.252      2.556      0.011         0.150     1.137\n",
       "70            -0.0409      0.169     -0.242      0.809        -0.372     0.290\n",
       "71            -0.1997      0.080     -2.494      0.013        -0.357    -0.043\n",
       "72            -0.6307      0.362     -1.742      0.082        -1.341     0.080\n",
       "73             0.1813      0.077      2.352      0.019         0.030     0.332\n",
       "74             0.1536      0.056      2.728      0.006         0.043     0.264\n",
       "75             0.0381      0.019      1.967      0.049         0.000     0.076\n",
       "76             0.1670      0.076      2.208      0.027         0.019     0.315\n",
       "77            -0.0435      0.031     -1.397      0.163        -0.105     0.018\n",
       "78             0.5242      0.379      1.383      0.167        -0.219     1.268\n",
       "79             0.0420      0.022      1.936      0.053        -0.001     0.084\n",
       "80            -0.0668      0.025     -2.679      0.007        -0.116    -0.018\n",
       "81            -0.0072      0.029     -0.249      0.803        -0.064     0.050\n",
       "82             0.0162      0.035      0.457      0.647        -0.053     0.086\n",
       "83            -0.0154      0.020     -0.757      0.449        -0.055     0.024\n",
       "84            -0.3838      0.203     -1.888      0.059        -0.782     0.015\n",
       "85             0.2935      0.308      0.953      0.341        -0.311     0.898\n",
       "86            -0.0045      0.167     -0.027      0.978        -0.331     0.322\n",
       "87            -0.2451      0.067     -3.673      0.000        -0.376    -0.114\n",
       "88            -0.0313      0.157     -0.199      0.842        -0.339     0.276\n",
       "89            -0.0511      0.086     -0.591      0.555        -0.221     0.118\n",
       "90             0.3623      0.130      2.786      0.005         0.107     0.617\n",
       "91             0.0553      0.033      1.701      0.089        -0.008     0.119\n",
       "92            -0.0377      0.034     -1.093      0.275        -0.105     0.030\n",
       "93            -0.0856      0.025     -3.482      0.001        -0.134    -0.037\n",
       "94             0.1333      0.069      1.944      0.052        -0.001     0.268\n",
       "95             0.1585      0.049      3.220      0.001         0.062     0.255\n",
       "96             0.1732      0.090      1.922      0.055        -0.004     0.350\n",
       "97             0.0265      0.041      0.642      0.521        -0.054     0.108\n",
       "98            -0.0155      0.058     -0.268      0.789        -0.129     0.098\n",
       "99             0.0314      0.038      0.823      0.411        -0.043     0.106\n",
       "100            0.0091      0.043      0.214      0.831        -0.075     0.093\n",
       "101            0.0574      0.343      0.167      0.867        -0.615     0.730\n",
       "102          -22.7480     14.173     -1.605      0.109       -50.545     5.049\n",
       "103            0.0751      0.083      0.902      0.367        -0.088     0.238\n",
       "104            0.5146      0.371      1.386      0.166        -0.213     1.243\n",
       "105           -0.1920      0.170     -1.130      0.259        -0.525     0.141\n",
       "106           -0.0618      0.145     -0.427      0.669        -0.345     0.222\n",
       "107            0.1832      0.087      2.116      0.034         0.013     0.353\n",
       "108           22.3362     14.149      1.579      0.115        -5.413    50.085\n",
       "109           -0.0894      0.045     -1.984      0.047        -0.178    -0.001\n",
       "110           -0.0085      0.082     -0.105      0.917        -0.168     0.151\n",
       "111            0.0345      0.144      0.240      0.811        -0.248     0.317\n",
       "112            0.0549      0.154      0.357      0.721        -0.247     0.357\n",
       "113            0.0626      0.047      1.339      0.181        -0.029     0.154\n",
       "114           -0.0217      0.203     -0.107      0.915        -0.420     0.376\n",
       "115            0.0123      0.207      0.060      0.952        -0.393     0.418\n",
       "116           -0.0146      0.042     -0.348      0.728        -0.097     0.068\n",
       "117           -0.0217      0.036     -0.601      0.548        -0.092     0.049\n",
       "118            0.0256      0.051      0.504      0.614        -0.074     0.125\n",
       "119            0.0044      0.031      0.144      0.886        -0.056     0.064\n",
       "120           -0.0441      0.023     -1.903      0.057        -0.090     0.001\n",
       "121            0.0940      0.081      1.162      0.245        -0.065     0.253\n",
       "122            0.1787      0.272      0.656      0.512        -0.355     0.713\n",
       "123           -0.0648      0.067     -0.972      0.331        -0.196     0.066\n",
       "124            0.0302      0.020      1.518      0.129        -0.009     0.069\n",
       "125           -0.0511      0.039     -1.319      0.187        -0.127     0.025\n",
       "==============================================================================\n",
       "Omnibus:                      389.508   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1263.113\n",
       "Skew:                           0.967   Prob(JB):                    5.23e-275\n",
       "Kurtosis:                       6.385   Cond. No.                     4.91e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.91e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runs linear regression and returns summary of model\n",
    "def reg_m(y, x):\n",
    "    model = sm.OLS(y, x.astype(float)).fit()\n",
    "    #fits simple ordinary least squares model\n",
    "    predictions = model.predict(x)\n",
    "    #makes predictions for y based on x\n",
    "    return(model.summary())\n",
    "reg_m(target, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x, target, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train MSE is 0.015565029548165637, the test MSE is 0.019588942468657524\n",
      "The train R^2 is 0.7144096927883361, the test R^2 is 0.6349076121953989\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multivariate Linear Regression with Sk Learn\"\"\"\n",
    "\n",
    "multi_regression_model = LinearRegression(fit_intercept=True, normalize = True)\n",
    "multi_regression_model.fit(x_train, y_train)\n",
    "\n",
    "train_MSE = np.mean((y_train - multi_regression_model.predict(x_train))**2)\n",
    "test_MSE = np.mean((y_test - multi_regression_model.predict(x_test))**2)\n",
    "print ('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = multi_regression_model.score(x_train, y_train)\n",
    "test_R_sq = multi_regression_model.score(x_test, y_test)\n",
    "print ('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train MSE is 0.015939528670830955, the test MSE is 0.019392751497559074\n",
      "The train R^2 is 0.7075383072145718, the test R^2 is 0.6385641561981515\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Linear least squares with l2 regularization.\"\"\"\n",
    "ridge_regression = Ridge(alpha=1.0, fit_intercept=True)\n",
    "ridge_regression.fit(x_train, y_train)\n",
    "\n",
    "train_MSE = np.mean((y_train - ridge_regression.predict(x_train))**2)\n",
    "test_MSE = np.mean((y_test - ridge_regression.predict(x_test))**2)\n",
    "print ('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = ridge_regression.score(x_train, y_train)\n",
    "test_R_sq = ridge_regression.score(x_test, y_test)\n",
    "print ('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print 'Ridge regression model:\\n {} + {}^T . x'.format(ridge_regression.intercept_, ridge_regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train MSE is 0.01643020496376541, the test MSE is 0.01939902425115551\n",
      "The train R^2 is 0.6985352794460089, the test R^2 is 0.6384472466410227\n"
     ]
    }
   ],
   "source": [
    "clf = BayesianRidge(alpha_1 = 6, alpha_2 = 1/25, lambda_1 = 6, lambda_2 = 1/25)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "train_MSE = np.mean((y_train - clf.predict(x_train))**2)\n",
    "test_MSE = np.mean((y_test - clf.predict(x_test))**2)\n",
    "print ('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = clf.score(x_train, y_train)\n",
    "test_R_sq = clf.score(x_test, y_test)\n",
    "print ('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train MSE is 0.03130122039137473, the test MSE is 0.030706615363592576\n",
      "The train R^2 is 0.4256788835504513, the test R^2 is 0.42770001277870073\n",
      "Lasso regression model:\n",
      " 0.26962628198172317 + [-0.00131359  0.         -0.          0.         -0.09486575  0.          0.\n",
      " -0.          0.          0.          0.          0.          0.         -0.\n",
      " -0.         -0.         -0.          0.          0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.          0.          0.          0.         -0.          0.         -0.\n",
      " -0.         -0.          0.         -0.          0.          0.          0.\n",
      "  0.          0.         -0.         -0.02875738 -0.         -0.         -0.\n",
      " -0.          0.          0.26750297  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.          0.\n",
      "  0.          0.         -0.         -0.          0.         -0.          0.\n",
      "  0.         -0.          0.         -0.         -0.          0.          0.\n",
      " -0.          0.          0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.          0.          0.         -0.          0.\n",
      "  0.          0.         -0.         -0.          0.         -0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]^T . x\n"
     ]
    }
   ],
   "source": [
    "lasso_regression = Lasso(alpha=0.02, fit_intercept=True)\n",
    "lasso_regression.fit(x_train, y_train)\n",
    "\n",
    "train_MSE = np.mean((y_train - lasso_regression.predict(x_train))**2)\n",
    "test_MSE = np.mean((y_test - lasso_regression.predict(x_test))**2)\n",
    "print ('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = lasso_regression.score(x_train, y_train)\n",
    "test_R_sq = lasso_regression.score(x_test, y_test)\n",
    "print ('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))\n",
    "print ('Lasso regression model:\\n {} + {}^T . x'.format(lasso_regression.intercept_, lasso_regression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of the y_train is approximately 6/25\n"
     ]
    }
   ],
   "source": [
    "#find out the mean of the y train, set the gamma parameters\n",
    "#run again and check for better results?\n",
    "from fractions import Fraction\n",
    "x=np.mean(y_train)\n",
    "print (\"the mean of the y_train is approximately 6/25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  3.  ,  0.03, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  5.  ,  0.04, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  2.  ,  0.01, ...,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [ 1.  ,  9.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  3.  ,  0.02, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  4.  ,  0.05, ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_cross_terms = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "#fit interaction terms using x train values\n",
    "cross_terms = gen_cross_terms.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train MSE with interaction terms is 0.0048158637569161765, the test MSE is 0.027020815744616775\n",
      "The train R^2 with interaction terms is 0.9116375586971309, the test R^2 is 0.49639475656153775\n"
     ]
    }
   ],
   "source": [
    "#Interaction Terms with Ridge Regression\n",
    "#create instance of PolynomialFeatures\n",
    "gen_cross_terms = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "#fit interaction terms using x train values\n",
    "cross_terms = gen_cross_terms.fit_transform(x_train)\n",
    "\n",
    "#combine x train terms with new interaction terms\n",
    "X_train_with_cross = np.hstack((x_train, cross_terms))\n",
    "\n",
    "#fit interaction terms using x test values\n",
    "cross_terms = gen_cross_terms.fit_transform(x_test)\n",
    "\n",
    "#combine x test terms with new interaction terms\n",
    "X_test_with_cross = np.hstack((x_test, cross_terms))\n",
    "\n",
    "#Create instance of ridge regression to use with train interaction terms \n",
    "ridge_regression_inter = Ridge(alpha=1.0, fit_intercept=True)\n",
    "\n",
    "#fit ridge regression with all x train and interaction terms and y train terms\n",
    "ridge_regression_inter.fit(X_train_with_cross, y_train)\n",
    "\n",
    "ridge_regression_inter.predict(X_test_with_cross)\n",
    "\n",
    "#Create instance of ridge regression to use with test interaction terms \n",
    "#ridge_regression_inter2 = Ridge(alpha=1.0, fit_intercept=True)\n",
    "\n",
    "#fit ridge regression with all x test and interaction terms and y test terms\n",
    "#ridge_regression_inter2.fit(X_test_with_cross, y_test)\n",
    "\n",
    "#calculate MSE and R squared values\n",
    "train_MSE = np.mean((y_train - ridge_regression_inter.predict(X_train_with_cross))**2)\n",
    "test_MSE = np.mean((y_test - ridge_regression_inter.predict(X_test_with_cross))**2)\n",
    "print ('The train MSE with interaction terms is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "train_R_sq = ridge_regression_inter.score(X_train_with_cross, y_train)\n",
    "test_R_sq = ridge_regression_inter.score(X_test_with_cross, y_test)\n",
    "print ('The train R^2 with interaction terms is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
