{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RationalQuadratic, DotProduct\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shapely\n",
    "\n",
    "import time\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "# from collections import Counter\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saved constants </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LU = pd.read_csv('boston_site_LU.csv')\n",
    "LU_width = LU.shape[1]\n",
    "col_names = ['Cropland', 'Golf Course',\n",
    "       'Saltwater Sandy Beach', 'cemetary', 'commercial', 'crop_land',\n",
    "       'forest', 'high_density_residential', 'industrial',\n",
    "       'low_density_residential', 'marina', 'medium_density_residential',\n",
    "       'mining', 'open_land', 'recreational', 'transitional', 'transportation',\n",
    "       'urban_public/institutional', 'waste', 'water', 'wetland', 'Sample Collection Start Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AQ = pd.read_csv('Appended_AQ_Data/')\n",
    "    \n",
    "    lat = AQ['Latitude']\n",
    "    lon = AQ['Longitude']\n",
    "    AQ['lat_lon'] = list(zip(lat, lon))\n",
    "    pts = AQ['lat_lon']\n",
    "    \n",
    "LU = pd.read_csv('boston_site_LU.csv')\n",
    "sites = LU['Site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = pd.read_csv('wunderground_csvs/weather_averages.csv')\n",
    "lat = W['latitude']\n",
    "lon = W['longitude']\n",
    "stations = list(zip(lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pollutants = ['CO', 'SO', 'NO', 'OZO', 'PM']\n",
    "local_path = 'Appended_AQ_Data/'\n",
    "\n",
    "alphas = [0.000888, 0.000000001, 0.000001, 0.00001, 0.000588]\n",
    "\n",
    "x_label = 'Time'\n",
    "y_labels = pollutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate grid vertices/ticks\n",
    "\n",
    "x = [ -71.20197, -70.96679]\n",
    "y = [42.291441, 42.420578]\n",
    "x_cell = 50\n",
    "y_cell = 50\n",
    "\n",
    "x_min = -71.20197\n",
    "x_max = -70.96679\n",
    "y_min = 42.291441\n",
    "y_max = 42.420578\n",
    "\n",
    "# create ticks\n",
    "x_s = np.linspace(x_min, x_max, x_cell + 1)\n",
    "y_s = np.linspace(y_min, y_max, y_cell + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define functions for land use data cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given lat/lon spot\n",
    "# return indicies for lat/lon grid box bounds\n",
    "# where bounds are:\n",
    "# y_s[lat_index - 1], y_s[lat_index]\n",
    "# x_s[lon_index - 1], x_s[lon_index]\n",
    "def find_gridbox(lat, long):\n",
    "    # locate correct grid box bounds\n",
    "    lat_index = 0\n",
    "    lon_index = 0\n",
    "    for i in range(0, 50):\n",
    "        if x_s[i] < long:\n",
    "            lon_index = i\n",
    "        if y_s[i] > lat:\n",
    "            lon_index = i\n",
    "    return (lat_index, lon_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse string from \"Grids\" column in boston_preds.csv\n",
    "# returns list of grid vertices as float64 tuples\n",
    "def parse_str(str_edit):\n",
    "    str_edit = str_edit.replace('[', '')\n",
    "    str_edit = str_edit.replace('(', '')\n",
    "    str_edit = str_edit.replace(']', '')\n",
    "    str_edit = str_edit.replace(')', '')\n",
    "    str_edit = str_edit.replace(',', '')\n",
    "    list = str_edit.split(' ')\n",
    "    return [(np.float64(list[0]), np.float64(list[1])), (np.float64(list[2]), np.float64(list[3])),\n",
    "            (np.float64(list[4]), np.float64(list[5])), (np.float64(list[6]), np.float64(list[7]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given lat, lon, and string containing cell verticies as tuples\n",
    "# return boolean value of whether or not lat/lon point is in cell\n",
    "\n",
    "def in_cell(lat, lon, cell):\n",
    "    cell_points = parse_str(cell)\n",
    "    lat_bool = lat > cell_points[0][0] and lat < cell_points[2][0]\n",
    "    lon_bool = lon > cell_points[0][1] and lon < cell_points[2][1]\n",
    "    if lat_bool and lon_bool:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adds land use columns to passed in dataframe,\n",
    "# returns edited version of dataframe\n",
    "\n",
    "def add_columns(df):\n",
    "    \n",
    "    df['Cropland'] = 0\n",
    "    df['Golf Course'] = 0\n",
    "    df['Saltwater Sandy Beach'] = 0\n",
    "    df['cemetary'] = 0\n",
    "    df['commercial'] = 0\n",
    "    df['crop_land'] = 0\n",
    "    df['forest'] = 0\n",
    "    df['high_density_residential'] = 0\n",
    "    df['industrial'] = 0\n",
    "    df['low_density_residential'] = 0\n",
    "    df['marina'] = 0\n",
    "    df['medium_density_residential'] = 0\n",
    "    df['mining'] = 0\n",
    "    df['open_land'] = 0\n",
    "    df['recreational'] = 0\n",
    "    df['transitional'] = 0\n",
    "    df['transportation'] = 0\n",
    "    df['urban_public/institutional'] = 0\n",
    "    df['waste'] = 0\n",
    "    df['water'] = 0\n",
    "    df['wetland'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AQ_df is the Boston AQ hourly\n",
    "# LU_df contains the cells and the landuse\n",
    "# AQ_index is the row of the cell that contains the point in the Boston AQ hourly df\n",
    "# LU_index is the row of the cell that contains the point in the cell and landuse df\n",
    "\n",
    "def update_proportions(AQ_df, LU_df, AQ_index, LU_index):\n",
    "    \n",
    "    for col in range(0, LU_width - 2):#df.loc[i, 'forest'] = df.loc[i, 'forest'] + my_dict['forest']\n",
    "        # print (AQ_index, AQ_width + col, LU_index, col + 1), \"\\n\"\n",
    "        # print AQ_df.iloc[AQ_index, col + AQ_width], \"LU:\", LU_df.iloc[LU_index, col+2]\n",
    "        AQ_df.iloc[AQ_index, col + AQ_width] = LU_df.iloc[LU_index, col + 2]\n",
    "    \n",
    "    return AQ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AQ_df is the Boston AQ hourly\n",
    "#LU_df contains the cells and the landuse\n",
    "#AQ_index is the row of the cell that contains the point in the Boston AQ hourly df\n",
    "#LU_index is the row of the cell that contains the point in the cell and landuse df\n",
    "\n",
    "def fill_in_weather(aq_df, weather_df, aq_index, weather_index):\n",
    "    \n",
    "    aq_df.iloc[aq_index, \"Weather\"] = weather_df.iloc[weather_index, \"weather\"]\n",
    "    \n",
    "    return aq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return numerical distance between two lat lon points\n",
    "def get_distance(aq_lat, aq_lon, weather_lat, weather_lon):\n",
    "    dist = math.sqrt((aq_lat - weather_lat)**2 + (aq_lon - weather_lon)**2)\n",
    "    return dist\n",
    "\n",
    "# given lat lon point, return index of row in weather_averages.csv file\n",
    "# that represents the closest sensor\n",
    "def find_sensor(aq_lat, aq_lon):\n",
    "    distances = []\n",
    "    for index in range(len(stations)):\n",
    "        distances[index] = get_distance(aq_lat, aq_lon, stations[i][0], stations[i][1])\n",
    "    return distances.index(min(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define functions needed for Gaussian process </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Cleans the times to a parsable format for multiple days in a row\"\"\"\n",
    "def clean_time_multiple(DF, col_name):\n",
    "    \n",
    "    times = []\n",
    "    time = list(DF[col_name])\n",
    "    \n",
    "    prev_T = float(time[0][:2])\n",
    "    day = 0\n",
    "    \n",
    "    for i in range(len(time)):\n",
    "        T = float(time[i][:2])\n",
    "        if prev_T > T:\n",
    "            day += 1\n",
    "        times.append(T + day*24)\n",
    "        prev_T = T\n",
    "    \n",
    "    DF[col_name] = times\n",
    "    \n",
    "    return DF\n",
    "\n",
    "\"\"\"Splits dataframe into train and test data\"\"\"\n",
    "def split_train_test(df):\n",
    "    train, test = train_test_split(df, test_size=.30, random_state=0)\n",
    "    x_train = train[\"Sample Collection Start Time\"]\n",
    "    y_train = train[\"Measure Value\"]\n",
    "    x_test = test[\"Sample Collection Start Time\"]\n",
    "    y_test = test[\"Measure Value\"]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\"\"\"\n",
    "Creates Gaussian model, computs r squarred, MSE and log liklihood\n",
    "Given x_train, y_train, x_test, and y_test data, and numerical alpha value\n",
    "Prints train and test R^2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def simple_gaussian(x_train, y_train, x_test, y_test, alpha):\n",
    "    \n",
    "    train_size = x_train.shape[0]\n",
    "    test_size = x_test.shape[0]\n",
    "    \n",
    "    kern = RBF(length_scale = 1)\n",
    "    \n",
    "    gp = GaussianProcessRegressor(kernel=kern, alpha=alpha, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=1, normalize_y=False, copy_X_train=False, random_state=None)\n",
    "    gp.fit(x_train.reshape(train_size, 1), y_train.reshape(train_size, 1))\n",
    "\n",
    "    y_train_pred, sigma_train = gp.predict(x_train.reshape(train_size, 1), return_std=True)\n",
    "    y_test_pred, sigma_test = gp.predict(x_test.reshape(test_size, 1), return_std=True)\n",
    "\n",
    "    # get R^2\n",
    "    r2 = gp.score(x_train.reshape(train_size, 1), y_train.reshape(train_size, 1))\n",
    "    r2_t = gp.score(x_test.reshape(test_size, 1), y_test.reshape(test_size, 1))\n",
    "\n",
    "    # get MSE measurements\n",
    "    MSE_test = np.mean((y_test_pred - y_test.reshape(test_size, 1))**2)\n",
    "    MSE_train = np.mean((y_train_pred - y_train.reshape(train_size, 1))**2)\n",
    "\n",
    "    # get log likelihood\n",
    "    t = gp.log_marginal_likelihood()\n",
    "\n",
    "    # calculate AIC\n",
    "    AIC = 2*len(x_test) - 2*np.log(-t)\n",
    "    \n",
    "    # print R^2 values\n",
    "    print('mean squared error of train data with model = ' + str(MSE_train))\n",
    "    print('mean squared error of test data with model = ' + str(MSE_test))\n",
    "    print('Akaike information criterion = ' + str(AIC))\n",
    "    print('log likelihood of model = ' + str(t))\n",
    "    print('training R^2 value = ' + str(r2))\n",
    "    print('testing R^2 value = ' + str(r2_t))\n",
    "    \n",
    "    return y_train_pred, y_test_pred, gp\n",
    "\n",
    "\"\"\"Plots predicted y values for testing set\"\"\"\n",
    "def plot_predictions(x_test, y_test, y_test_pred, x_label, y_label):\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(x_test, y_test, '.', color=\"b\",)\n",
    "    plt.scatter(x_test, y_test_pred, color='r')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(y_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Code to combine AQ data file with land use data and implement Gaussian process</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: this cell takes a long time\n",
    "# for each pollutant, prepare a cleaned dataframe with land use\n",
    "# dataframes list ends up with order: 'CO', 'SO', 'NO', 'OZO', 'PM'\n",
    "\n",
    "sites = LU['Site']\n",
    "dataframes = np.zeros(5, dtype=pd.DataFrame)\n",
    "\n",
    "for i in range(0,5):\n",
    "\n",
    "    filename = local_path + 'appended' + pollutants[i] + '.csv'\n",
    "    \n",
    "    AQ = pd.read_csv(filename)\n",
    "    \n",
    "    lat = AQ['Latitude']\n",
    "    lon = AQ['Longitude']\n",
    "    AQ['lat_lon'] = list(zip(lat, lon))\n",
    "    pts = AQ['lat_lon']\n",
    "    \n",
    "    AQ_width = AQ.shape[1]\n",
    "\n",
    "    AQ = add_columns(AQ)\n",
    "\n",
    "    for pt in range(len(AQ['lat_lon'])):\n",
    "        for cell in range(len(sites)):\n",
    "            if in_cell(pts[pt][1], pts[pt][0], LU['Site'][cell]):\n",
    "                # add cell info to appendedAQ.csv file\n",
    "                AQ = update_proportions(AQ, LU, pt, cell)\n",
    "    \n",
    "    dataframes[i] = AQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54e6031402b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mAQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "# for each pollutant, do a gaussian process\n",
    "\n",
    "for i in range(0,5):\n",
    "    \n",
    "    AQ = dataframes[i]\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_train_test(AQ)\n",
    "\n",
    "    y_train_pred, y_test_pred, gp = simple_gaussian(x_train, y_train, x_test, y_test, alphas[i])\n",
    "\n",
    "    plot_predictions(x_test, y_test, y_test_pred, x_label, y_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
