{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "sns.set(style=\"ticks\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2 = pd.read_csv(\"finished_no2weather.csv\")\n",
    "so2 = pd.read_csv(\"finished_so2weather.csv\")\n",
    "\n",
    "#scale data\n",
    "no2_scaled = no2.iloc[:, 3:]\n",
    "no2_scaled = preprocessing.minmax_scale(no2_scaled)\n",
    "no2_scaled = pd.DataFrame(no2_scaled)\n",
    "add = no2.iloc[:, :3]\n",
    "no2_scaled = add.join(no2_scaled)\n",
    "cols = list(no2.columns)\n",
    "no2_scaled.columns = cols\n",
    "\n",
    "#splits train, test sets\n",
    "train_no2, test_no2 = train_test_split(no2_scaled, test_size=.30, random_state=0)\n",
    "#Scales y data (ppm values)\n",
    "y_train_no2 = train_no2['Arithmetic Mean'].values * 100\n",
    "y_test_no2 = test_no2['Arithmetic Mean'].values * 100\n",
    "test_no2 = test_no2.iloc[:, 3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['forest',\n",
    "       'open_land', 'water', 'wetland', 'saltwater/sandybeach', 'recreational',\n",
    "       'marina', 'transitional', 'urban_public/institution', 'commercial',\n",
    "       'transportation', 'mining', 'waste', 'crop_land', 'cemetary',\n",
    "       'golfcourse', 'high_density_residential', 'medium_density_residential',\n",
    "       'low_density_residential', 'industrial', 'Dew Point',\n",
    "       'Outdoor Temperature', 'Solar radiation', 'Sulfation rate',\n",
    "       'Wind Speed - Resultant']\n",
    "\n",
    "\n",
    "\"\"\"Input: y data set, list of predictor variables\n",
    "Action: X data set formed, runs a MLR through data set, computes AIC, deletes predictor with maximum AIC\n",
    "returns: models - list of remaining predictors after each iteration\n",
    "\"\"\"\n",
    "def backwards_var_sel_aic(y, all_predictors, train):\n",
    "    preds = [] #to hold all the list of predictors\n",
    "    copy = all_predictors #keeps copy of full predictors list #holds AIC values for each iteration\n",
    "    models = []\n",
    "    indices = []\n",
    "    for j in range(len(copy)):\n",
    "        t = all_predictors\n",
    "        removed = []\n",
    "        AIC = []\n",
    "        for i in range(len(t)):\n",
    "            #removes one predictor in each iteration\n",
    "            predictors = all_predictors[:i] + all_predictors[i+1 :]\n",
    "            #print(predictors)\n",
    "            #train and fit a model on those predictors\n",
    "            x = train[predictors].values\n",
    "            x = add_constant(x)\n",
    "            OLS_model = sm.OLS(y, x).fit()\n",
    "\n",
    "            #append AIC value to AIC list for each iteration\n",
    "            AIC.append(OLS_model.aic)\n",
    "        #finds the largest AIC and the index at which it exists\n",
    "        worstAIC = max(AIC)\n",
    "        index = AIC.index(max(AIC))\n",
    "        worstLU = all_predictors[index]\n",
    "        indices.append(copy.index(worstLU))\n",
    "        #reassign all_predictors to a new list with the predictor corresponding to the worst AIC removed\n",
    "        all_predictors = all_predictors[:index] + all_predictors[index+1 :]\n",
    "        #append a list of the remaining predictors to the models list\n",
    "        preds.append(all_predictors)\n",
    "        models.append(OLS_model)\n",
    "        \n",
    "    return models, indices, preds\n",
    "\n",
    "\n",
    "models, indices, predicts = backwards_var_sel_aic(y_train_no2, predictors, train_no2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Annamira/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:978: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "/Users/Annamira/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/Annamira/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:875: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/Annamira/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1814: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   67.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 31 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>2.17e-71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:26:41</td>     <th>  Log-Likelihood:    </th> <td> -345.20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   217</td>      <th>  AIC:               </th> <td>   722.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   201</td>      <th>  BIC:               </th> <td>   776.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    16</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0479</td> <td>    1.275</td> <td>   -0.038</td> <td> 0.970</td> <td>   -2.562     2.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.5719</td> <td>    1.548</td> <td>   -0.369</td> <td> 0.712</td> <td>   -3.625     2.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0863</td> <td>    1.340</td> <td>   -0.064</td> <td> 0.949</td> <td>   -2.728     2.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -6.5816</td> <td>    3.819</td> <td>   -1.723</td> <td> 0.086</td> <td>  -14.112     0.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 7.937e-16</td> <td> 3.21e-15</td> <td>    0.247</td> <td> 0.805</td> <td>-5.54e-15  7.13e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1.652e-15</td> <td>  2.1e-15</td> <td>   -0.788</td> <td> 0.432</td> <td>-5.79e-15  2.48e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>-3.829e-15</td> <td> 3.08e-15</td> <td>   -1.243</td> <td> 0.215</td> <td> -9.9e-15  2.24e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.8287</td> <td>    1.907</td> <td>    0.435</td> <td> 0.664</td> <td>   -2.931     4.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    2.0670</td> <td>    1.193</td> <td>    1.733</td> <td> 0.085</td> <td>   -0.285     4.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    1.8598</td> <td>    1.395</td> <td>    1.333</td> <td> 0.184</td> <td>   -0.891     4.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    2.8184</td> <td>    1.421</td> <td>    1.983</td> <td> 0.049</td> <td>    0.016     5.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.3114</td> <td>    1.601</td> <td>   -0.194</td> <td> 0.846</td> <td>   -3.469     2.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>-2.137e-15</td> <td> 1.33e-15</td> <td>   -1.602</td> <td> 0.111</td> <td>-4.77e-15  4.93e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0727</td> <td>    1.423</td> <td>   -0.051</td> <td> 0.959</td> <td>   -2.878     2.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>-4.989e-16</td> <td> 4.76e-16</td> <td>   -1.049</td> <td> 0.295</td> <td>-1.44e-15  4.39e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>-1.289e-17</td> <td> 7.36e-18</td> <td>   -1.751</td> <td> 0.081</td> <td>-2.74e-17  1.62e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0         0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    1.6353</td> <td>    1.374</td> <td>    1.190</td> <td> 0.235</td> <td>   -1.074     4.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0         0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    3.0438</td> <td>    1.460</td> <td>    2.085</td> <td> 0.038</td> <td>    0.165     5.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.4941</td> <td>    1.498</td> <td>   -0.330</td> <td> 0.742</td> <td>   -3.448     2.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.5068</td> <td>    0.713</td> <td>    0.711</td> <td> 0.478</td> <td>   -0.899     1.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.5403</td> <td>    1.578</td> <td>    0.343</td> <td> 0.732</td> <td>   -2.570     3.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.7813</td> <td>    1.231</td> <td>    0.635</td> <td> 0.526</td> <td>   -1.647     3.209</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>39.192</td> <th>  Durbin-Watson:     </th> <td>   2.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  65.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.970</td> <th>  Prob(JB):          </th> <td>6.67e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.860</td> <th>  Cond. No.          </th> <td>8.61e+33</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.842\n",
       "Model:                            OLS   Adj. R-squared:                  0.830\n",
       "Method:                 Least Squares   F-statistic:                     67.02\n",
       "Date:                Mon, 31 Jul 2017   Prob (F-statistic):           2.17e-71\n",
       "Time:                        09:26:41   Log-Likelihood:                -345.20\n",
       "No. Observations:                 217   AIC:                             722.4\n",
       "Df Residuals:                     201   BIC:                             776.5\n",
       "Df Model:                          16                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.0479      1.275     -0.038      0.970        -2.562     2.466\n",
       "x2            -0.5719      1.548     -0.369      0.712        -3.625     2.481\n",
       "x3            -0.0863      1.340     -0.064      0.949        -2.728     2.556\n",
       "x4            -6.5816      3.819     -1.723      0.086       -14.112     0.949\n",
       "const       7.937e-16   3.21e-15      0.247      0.805     -5.54e-15  7.13e-15\n",
       "x5         -1.652e-15    2.1e-15     -0.788      0.432     -5.79e-15  2.48e-15\n",
       "x6         -3.829e-15   3.08e-15     -1.243      0.215      -9.9e-15  2.24e-15\n",
       "x7             0.8287      1.907      0.435      0.664        -2.931     4.589\n",
       "x8             2.0670      1.193      1.733      0.085        -0.285     4.419\n",
       "x9             1.8598      1.395      1.333      0.184        -0.891     4.610\n",
       "x10            2.8184      1.421      1.983      0.049         0.016     5.621\n",
       "x11           -0.3114      1.601     -0.194      0.846        -3.469     2.846\n",
       "x12        -2.137e-15   1.33e-15     -1.602      0.111     -4.77e-15  4.93e-16\n",
       "x13           -0.0727      1.423     -0.051      0.959        -2.878     2.733\n",
       "x14        -4.989e-16   4.76e-16     -1.049      0.295     -1.44e-15  4.39e-16\n",
       "x15        -1.289e-17   7.36e-18     -1.751      0.081     -2.74e-17  1.62e-18\n",
       "x16                 0          0        nan        nan             0         0\n",
       "x17            1.6353      1.374      1.190      0.235        -1.074     4.344\n",
       "x18                 0          0        nan        nan             0         0\n",
       "x19            3.0438      1.460      2.085      0.038         0.165     5.922\n",
       "x20           -0.4941      1.498     -0.330      0.742        -3.448     2.460\n",
       "x21            0.5068      0.713      0.711      0.478        -0.899     1.913\n",
       "x22            0.5403      1.578      0.343      0.732        -2.570     3.651\n",
       "x23            0.7813      1.231      0.635      0.526        -1.647     3.209\n",
       "==============================================================================\n",
       "Omnibus:                       39.192   Durbin-Watson:                   2.159\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               65.282\n",
       "Skew:                           0.970   Prob(JB):                     6.67e-15\n",
       "Kurtosis:                       4.860   Cond. No.                     8.61e+33\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.69e-66. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relevant for \n",
    "best_models = []\n",
    "for i in range(len(models)):\n",
    "    #find all models where the average pvalue is less than 0.09\n",
    "    best_models.append(models[i].aic)\n",
    "m = min(best_models)\n",
    "#finds minimum aic among each model\n",
    "#gets the index of every instance where the minimum AIC occurs\n",
    "index_of_model = best_models.index(m)\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
