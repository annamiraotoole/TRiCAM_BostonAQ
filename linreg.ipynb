{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#For reading and manipulating tabular data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#For basic statistical modeling\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ucidata.csv', sep=\",\", header=None)\n",
    "\n",
    "#replaces question marks with \"0\"\n",
    "for c in range(0, 127):\n",
    "    for r in range(0, 1994):\n",
    "        if df[c][r] == \"?\":\n",
    "            df.set_value(r, c, 0)\n",
    "            \n",
    "#turns all columns that are numerics encoded as strings into floats\n",
    "for c in range(4, 127):\n",
    "    for r in range(0, 1994):\n",
    "        if type(df[c][r]) == str:\n",
    "            df.set_value(r, c, float(df[c][r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#takes out features columns\n",
    "x = df.iloc[:, 4:126]\n",
    "#x = sm.add_constant(x)\n",
    "#takes of what we want to predict\n",
    "target = df.iloc[:, 127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395, 122) (599, 122)\n"
     ]
    }
   ],
   "source": [
    "#First break up your data into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, target, test_size=0.30, random_state=0)\n",
    "print (x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is of form $Y = X\\beta + \\mu$, where $\\mu\\sim N\\left(0,\\Sigma\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>127</td>       <th>  R-squared:         </th> <td>   0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   64.43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 29 Jun 2017</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:13:41</td>     <th>  Log-Likelihood:    </th> <td>  922.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1395</td>      <th>  AIC:               </th> <td>  -1600.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1273</td>      <th>  BIC:               </th> <td>  -960.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   122</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>   -0.0028</td> <td>    0.001</td> <td>   -2.171</td> <td> 0.030</td> <td>   -0.005    -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.2655</td> <td>    0.477</td> <td>    0.557</td> <td> 0.578</td> <td>   -0.670     1.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0628</td> <td>    0.103</td> <td>    0.608</td> <td> 0.544</td> <td>   -0.140     0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.2417</td> <td>    0.064</td> <td>    3.787</td> <td> 0.000</td> <td>    0.116     0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0066</td> <td>    0.073</td> <td>    0.091</td> <td> 0.928</td> <td>   -0.136     0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>   -0.0180</td> <td>    0.043</td> <td>   -0.422</td> <td> 0.673</td> <td>   -0.101     0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0938</td> <td>    0.064</td> <td>    1.465</td> <td> 0.143</td> <td>   -0.032     0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>    0.1548</td> <td>    0.124</td> <td>    1.246</td> <td> 0.213</td> <td>   -0.089     0.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>  <td>   -0.1347</td> <td>    0.184</td> <td>   -0.733</td> <td> 0.464</td> <td>   -0.495     0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>  <td>   -0.2221</td> <td>    0.196</td> <td>   -1.131</td> <td> 0.258</td> <td>   -0.607     0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th>  <td>    0.1749</td> <td>    0.116</td> <td>    1.503</td> <td> 0.133</td> <td>   -0.053     0.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>   -0.3429</td> <td>    0.470</td> <td>   -0.729</td> <td> 0.466</td> <td>   -1.265     0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>  <td>    0.0540</td> <td>    0.019</td> <td>    2.831</td> <td> 0.005</td> <td>    0.017     0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>  <td>   -0.2114</td> <td>    0.208</td> <td>   -1.017</td> <td> 0.309</td> <td>   -0.619     0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>   -0.0981</td> <td>    0.102</td> <td>   -0.966</td> <td> 0.334</td> <td>   -0.297     0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>  <td>    0.0419</td> <td>    0.024</td> <td>    1.748</td> <td> 0.081</td> <td>   -0.005     0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.1582</td> <td>    0.084</td> <td>   -1.888</td> <td> 0.059</td> <td>   -0.323     0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>  <td>    0.1544</td> <td>    0.129</td> <td>    1.200</td> <td> 0.230</td> <td>   -0.098     0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>  <td>    0.0943</td> <td>    0.056</td> <td>    1.689</td> <td> 0.092</td> <td>   -0.015     0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>  <td>   -0.0668</td> <td>    0.045</td> <td>   -1.490</td> <td> 0.137</td> <td>   -0.155     0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.2376</td> <td>    0.195</td> <td>    1.221</td> <td> 0.222</td> <td>   -0.144     0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>    0.0999</td> <td>    0.243</td> <td>    0.410</td> <td> 0.682</td> <td>   -0.378     0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.2413</td> <td>    0.208</td> <td>   -1.162</td> <td> 0.245</td> <td>   -0.649     0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>   -0.0224</td> <td>    0.031</td> <td>   -0.730</td> <td> 0.466</td> <td>   -0.083     0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.0353</td> <td>    0.023</td> <td>   -1.506</td> <td> 0.132</td> <td>   -0.081     0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.0321</td> <td>    0.024</td> <td>    1.355</td> <td> 0.176</td> <td>   -0.014     0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>  <td>    0.0300</td> <td>    0.023</td> <td>    1.325</td> <td> 0.186</td> <td>   -0.014     0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>    0.0235</td> <td>    0.030</td> <td>    0.789</td> <td> 0.430</td> <td>   -0.035     0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.0920</td> <td>    0.190</td> <td>    0.484</td> <td> 0.628</td> <td>   -0.281     0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>  <td>   -0.1554</td> <td>    0.074</td> <td>   -2.113</td> <td> 0.035</td> <td>   -0.300    -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.0831</td> <td>    0.083</td> <td>   -0.999</td> <td> 0.318</td> <td>   -0.246     0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>    0.0287</td> <td>    0.115</td> <td>    0.249</td> <td> 0.803</td> <td>   -0.197     0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>    0.0524</td> <td>    0.094</td> <td>    0.559</td> <td> 0.576</td> <td>   -0.132     0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>  <td>    0.0368</td> <td>    0.049</td> <td>    0.757</td> <td> 0.449</td> <td>   -0.058     0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.3446</td> <td>    0.091</td> <td>    3.785</td> <td> 0.000</td> <td>    0.166     0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>   -0.0972</td> <td>    0.040</td> <td>   -2.457</td> <td> 0.014</td> <td>   -0.175    -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>  <td>   -0.0544</td> <td>    0.050</td> <td>   -1.094</td> <td> 0.274</td> <td>   -0.152     0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>    0.1239</td> <td>    0.066</td> <td>    1.866</td> <td> 0.062</td> <td>   -0.006     0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>    0.1370</td> <td>    0.106</td> <td>    1.299</td> <td> 0.194</td> <td>   -0.070     0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>43</th>  <td>    0.3985</td> <td>    0.285</td> <td>    1.397</td> <td> 0.163</td> <td>   -0.161     0.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>  <td>    0.2197</td> <td>    0.084</td> <td>    2.607</td> <td> 0.009</td> <td>    0.054     0.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>45</th>  <td>   -0.0060</td> <td>    0.347</td> <td>   -0.017</td> <td> 0.986</td> <td>   -0.687     0.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>   -0.3245</td> <td>    0.587</td> <td>   -0.553</td> <td> 0.581</td> <td>   -1.477     0.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0387</td> <td>    0.207</td> <td>   -0.187</td> <td> 0.851</td> <td>   -0.444     0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>  <td>   -0.0840</td> <td>    0.193</td> <td>   -0.435</td> <td> 0.664</td> <td>   -0.463     0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>   -0.2648</td> <td>    0.186</td> <td>   -1.425</td> <td> 0.155</td> <td>   -0.629     0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.0305</td> <td>    0.058</td> <td>    0.528</td> <td> 0.597</td> <td>   -0.083     0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.0054</td> <td>    0.051</td> <td>    0.106</td> <td> 0.916</td> <td>   -0.094     0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.0977</td> <td>    0.056</td> <td>    1.746</td> <td> 0.081</td> <td>   -0.012     0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>53</th>  <td>   -0.2458</td> <td>    0.064</td> <td>   -3.838</td> <td> 0.000</td> <td>   -0.371    -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.2707</td> <td>    0.188</td> <td>   -1.436</td> <td> 0.151</td> <td>   -0.640     0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>    0.1233</td> <td>    0.057</td> <td>    2.163</td> <td> 0.031</td> <td>    0.011     0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>56</th>  <td>   -0.1620</td> <td>    0.117</td> <td>   -1.390</td> <td> 0.165</td> <td>   -0.391     0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.0315</td> <td>    0.051</td> <td>   -0.616</td> <td> 0.538</td> <td>   -0.132     0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>  <td>    0.1263</td> <td>    0.079</td> <td>    1.595</td> <td> 0.111</td> <td>   -0.029     0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>59</th>  <td>   -0.1238</td> <td>    0.089</td> <td>   -1.387</td> <td> 0.166</td> <td>   -0.299     0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0650</td> <td>    0.071</td> <td>    0.915</td> <td> 0.360</td> <td>   -0.074     0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>    0.0468</td> <td>    0.156</td> <td>    0.300</td> <td> 0.764</td> <td>   -0.259     0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.2570</td> <td>    0.275</td> <td>   -0.936</td> <td> 0.349</td> <td>   -0.796     0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.6461</td> <td>    0.335</td> <td>    1.926</td> <td> 0.054</td> <td>   -0.012     1.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.4027</td> <td>    0.276</td> <td>   -1.459</td> <td> 0.145</td> <td>   -0.944     0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>   -0.0497</td> <td>    0.078</td> <td>   -0.634</td> <td> 0.526</td> <td>   -0.204     0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>  <td>   -0.1776</td> <td>    0.084</td> <td>   -2.114</td> <td> 0.035</td> <td>   -0.342    -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>  <td>   -0.2225</td> <td>    0.280</td> <td>   -0.794</td> <td> 0.427</td> <td>   -0.772     0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>  <td>   -0.0577</td> <td>    0.288</td> <td>   -0.201</td> <td> 0.841</td> <td>   -0.622     0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>69</th>  <td>    0.4222</td> <td>    0.307</td> <td>    1.376</td> <td> 0.169</td> <td>   -0.180     1.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>    0.0760</td> <td>    0.206</td> <td>    0.369</td> <td> 0.712</td> <td>   -0.328     0.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>   -0.1405</td> <td>    0.098</td> <td>   -1.431</td> <td> 0.153</td> <td>   -0.333     0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>  <td>   -0.5182</td> <td>    0.442</td> <td>   -1.172</td> <td> 0.241</td> <td>   -1.385     0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>73</th>  <td>    0.1958</td> <td>    0.089</td> <td>    2.211</td> <td> 0.027</td> <td>    0.022     0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>    0.2134</td> <td>    0.068</td> <td>    3.146</td> <td> 0.002</td> <td>    0.080     0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>75</th>  <td>    0.0431</td> <td>    0.024</td> <td>    1.815</td> <td> 0.070</td> <td>   -0.003     0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>    0.1912</td> <td>    0.086</td> <td>    2.213</td> <td> 0.027</td> <td>    0.022     0.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>   -0.0209</td> <td>    0.037</td> <td>   -0.572</td> <td> 0.568</td> <td>   -0.093     0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>78</th>  <td>    0.4918</td> <td>    0.462</td> <td>    1.065</td> <td> 0.287</td> <td>   -0.414     1.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>    0.0811</td> <td>    0.027</td> <td>    3.034</td> <td> 0.002</td> <td>    0.029     0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.0553</td> <td>    0.030</td> <td>   -1.866</td> <td> 0.062</td> <td>   -0.113     0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>   -0.0037</td> <td>    0.035</td> <td>   -0.106</td> <td> 0.916</td> <td>   -0.073     0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>   -0.0177</td> <td>    0.043</td> <td>   -0.412</td> <td> 0.680</td> <td>   -0.102     0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>   -0.0441</td> <td>    0.025</td> <td>   -1.735</td> <td> 0.083</td> <td>   -0.094     0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>   -0.4555</td> <td>    0.258</td> <td>   -1.768</td> <td> 0.077</td> <td>   -0.961     0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>    0.5191</td> <td>    0.393</td> <td>    1.319</td> <td> 0.187</td> <td>   -0.253     1.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>   -0.1478</td> <td>    0.205</td> <td>   -0.719</td> <td> 0.472</td> <td>   -0.551     0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>   -0.2772</td> <td>    0.084</td> <td>   -3.298</td> <td> 0.001</td> <td>   -0.442    -0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>    0.0832</td> <td>    0.193</td> <td>    0.431</td> <td> 0.667</td> <td>   -0.296     0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0710</td> <td>    0.109</td> <td>   -0.649</td> <td> 0.516</td> <td>   -0.286     0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>90</th>  <td>    0.3356</td> <td>    0.152</td> <td>    2.206</td> <td> 0.028</td> <td>    0.037     0.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>  <td>    0.0778</td> <td>    0.041</td> <td>    1.899</td> <td> 0.058</td> <td>   -0.003     0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.0662</td> <td>    0.042</td> <td>   -1.592</td> <td> 0.112</td> <td>   -0.148     0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>   -0.1115</td> <td>    0.030</td> <td>   -3.759</td> <td> 0.000</td> <td>   -0.170    -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>  <td>    0.0898</td> <td>    0.092</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.090     0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>    0.1322</td> <td>    0.059</td> <td>    2.251</td> <td> 0.025</td> <td>    0.017     0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>96</th>  <td>    0.0661</td> <td>    0.109</td> <td>    0.604</td> <td> 0.546</td> <td>   -0.149     0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>    0.0460</td> <td>    0.052</td> <td>    0.883</td> <td> 0.377</td> <td>   -0.056     0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>   -0.0559</td> <td>    0.072</td> <td>   -0.781</td> <td> 0.435</td> <td>   -0.196     0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0180</td> <td>    0.046</td> <td>    0.391</td> <td> 0.696</td> <td>   -0.072     0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0014</td> <td>    0.052</td> <td>   -0.026</td> <td> 0.979</td> <td>   -0.104     0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>    0.1096</td> <td>    0.405</td> <td>    0.271</td> <td> 0.787</td> <td>   -0.685     0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>  -19.2666</td> <td>   14.465</td> <td>   -1.332</td> <td> 0.183</td> <td>  -47.645     9.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.0777</td> <td>    0.099</td> <td>    0.783</td> <td> 0.434</td> <td>   -0.117     0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.5621</td> <td>    0.426</td> <td>    1.318</td> <td> 0.188</td> <td>   -0.275     1.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>105</th> <td>   -0.1884</td> <td>    0.201</td> <td>   -0.937</td> <td> 0.349</td> <td>   -0.583     0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>106</th> <td>    0.0089</td> <td>    0.178</td> <td>    0.050</td> <td> 0.960</td> <td>   -0.341     0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.1616</td> <td>    0.111</td> <td>    1.458</td> <td> 0.145</td> <td>   -0.056     0.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>108</th> <td>   18.8439</td> <td>   14.441</td> <td>    1.305</td> <td> 0.192</td> <td>   -9.486    47.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>109</th> <td>   -0.1156</td> <td>    0.053</td> <td>   -2.178</td> <td> 0.030</td> <td>   -0.220    -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>110</th> <td>   -0.0365</td> <td>    0.094</td> <td>   -0.390</td> <td> 0.696</td> <td>   -0.220     0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>    0.1482</td> <td>    0.173</td> <td>    0.856</td> <td> 0.392</td> <td>   -0.192     0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.2604</td> <td>    0.188</td> <td>    1.389</td> <td> 0.165</td> <td>   -0.107     0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>113</th> <td>    0.0682</td> <td>    0.057</td> <td>    1.195</td> <td> 0.232</td> <td>   -0.044     0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114</th> <td>   -0.2227</td> <td>    0.250</td> <td>   -0.892</td> <td> 0.373</td> <td>   -0.712     0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>115</th> <td>    0.1087</td> <td>    0.257</td> <td>    0.424</td> <td> 0.672</td> <td>   -0.395     0.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>116</th> <td>    0.0333</td> <td>    0.047</td> <td>    0.704</td> <td> 0.482</td> <td>   -0.059     0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>117</th> <td>   -0.0177</td> <td>    0.042</td> <td>   -0.416</td> <td> 0.678</td> <td>   -0.101     0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>118</th> <td>    0.0682</td> <td>    0.070</td> <td>    0.971</td> <td> 0.332</td> <td>   -0.070     0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>119</th> <td>    0.0076</td> <td>    0.038</td> <td>    0.201</td> <td> 0.840</td> <td>   -0.066     0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>120</th> <td>   -0.0212</td> <td>    0.029</td> <td>   -0.742</td> <td> 0.458</td> <td>   -0.077     0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>121</th> <td>    0.0038</td> <td>    0.090</td> <td>    0.042</td> <td> 0.967</td> <td>   -0.173     0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>122</th> <td>    0.1911</td> <td>    0.310</td> <td>    0.617</td> <td> 0.537</td> <td>   -0.417     0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>123</th> <td>   -0.0352</td> <td>    0.082</td> <td>   -0.427</td> <td> 0.670</td> <td>   -0.197     0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>124</th> <td>    0.0173</td> <td>    0.023</td> <td>    0.744</td> <td> 0.457</td> <td>   -0.028     0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>125</th> <td>   -0.0863</td> <td>    0.048</td> <td>   -1.797</td> <td> 0.073</td> <td>   -0.180     0.008</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>233.220</td> <th>  Durbin-Watson:     </th> <td>   1.934</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 665.128</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.860</td>  <th>  Prob(JB):          </th> <td>3.71e-145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.912</td>  <th>  Cond. No.          </th> <td>4.24e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    127   R-squared:                       0.861\n",
       "Model:                            OLS   Adj. R-squared:                  0.847\n",
       "Method:                 Least Squares   F-statistic:                     64.43\n",
       "Date:                Thu, 29 Jun 2017   Prob (F-statistic):               0.00\n",
       "Time:                        09:13:41   Log-Likelihood:                 922.02\n",
       "No. Observations:                1395   AIC:                            -1600.\n",
       "Df Residuals:                    1273   BIC:                            -960.7\n",
       "Df Model:                         122                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "4             -0.0028      0.001     -2.171      0.030        -0.005    -0.000\n",
       "5              0.2655      0.477      0.557      0.578        -0.670     1.201\n",
       "6              0.0628      0.103      0.608      0.544        -0.140     0.266\n",
       "7              0.2417      0.064      3.787      0.000         0.116     0.367\n",
       "8              0.0066      0.073      0.091      0.928        -0.136     0.149\n",
       "9             -0.0180      0.043     -0.422      0.673        -0.101     0.066\n",
       "10             0.0938      0.064      1.465      0.143        -0.032     0.220\n",
       "11             0.1548      0.124      1.246      0.213        -0.089     0.398\n",
       "12            -0.1347      0.184     -0.733      0.464        -0.495     0.226\n",
       "13            -0.2221      0.196     -1.131      0.258        -0.607     0.163\n",
       "14             0.1749      0.116      1.503      0.133        -0.053     0.403\n",
       "15            -0.3429      0.470     -0.729      0.466        -1.265     0.579\n",
       "16             0.0540      0.019      2.831      0.005         0.017     0.091\n",
       "17            -0.2114      0.208     -1.017      0.309        -0.619     0.196\n",
       "18            -0.0981      0.102     -0.966      0.334        -0.297     0.101\n",
       "19             0.0419      0.024      1.748      0.081        -0.005     0.089\n",
       "20            -0.1582      0.084     -1.888      0.059        -0.323     0.006\n",
       "21             0.1544      0.129      1.200      0.230        -0.098     0.407\n",
       "22             0.0943      0.056      1.689      0.092        -0.015     0.204\n",
       "23            -0.0668      0.045     -1.490      0.137        -0.155     0.021\n",
       "24             0.2376      0.195      1.221      0.222        -0.144     0.619\n",
       "25             0.0999      0.243      0.410      0.682        -0.378     0.578\n",
       "26            -0.2413      0.208     -1.162      0.245        -0.649     0.166\n",
       "27            -0.0224      0.031     -0.730      0.466        -0.083     0.038\n",
       "28            -0.0353      0.023     -1.506      0.132        -0.081     0.011\n",
       "29             0.0321      0.024      1.355      0.176        -0.014     0.079\n",
       "30             0.0300      0.023      1.325      0.186        -0.014     0.074\n",
       "31             0.0235      0.030      0.789      0.430        -0.035     0.082\n",
       "32             0.0920      0.190      0.484      0.628        -0.281     0.465\n",
       "33            -0.1554      0.074     -2.113      0.035        -0.300    -0.011\n",
       "34            -0.0831      0.083     -0.999      0.318        -0.246     0.080\n",
       "35             0.0287      0.115      0.249      0.803        -0.197     0.254\n",
       "36             0.0524      0.094      0.559      0.576        -0.132     0.236\n",
       "37             0.0368      0.049      0.757      0.449        -0.058     0.132\n",
       "38             0.3446      0.091      3.785      0.000         0.166     0.523\n",
       "39            -0.0972      0.040     -2.457      0.014        -0.175    -0.020\n",
       "40            -0.0544      0.050     -1.094      0.274        -0.152     0.043\n",
       "41             0.1239      0.066      1.866      0.062        -0.006     0.254\n",
       "42             0.1370      0.106      1.299      0.194        -0.070     0.344\n",
       "43             0.3985      0.285      1.397      0.163        -0.161     0.958\n",
       "44             0.2197      0.084      2.607      0.009         0.054     0.385\n",
       "45            -0.0060      0.347     -0.017      0.986        -0.687     0.675\n",
       "46            -0.3245      0.587     -0.553      0.581        -1.477     0.827\n",
       "47            -0.0387      0.207     -0.187      0.851        -0.444     0.367\n",
       "48            -0.0840      0.193     -0.435      0.664        -0.463     0.295\n",
       "49            -0.2648      0.186     -1.425      0.155        -0.629     0.100\n",
       "50             0.0305      0.058      0.528      0.597        -0.083     0.144\n",
       "51             0.0054      0.051      0.106      0.916        -0.094     0.105\n",
       "52             0.0977      0.056      1.746      0.081        -0.012     0.208\n",
       "53            -0.2458      0.064     -3.838      0.000        -0.371    -0.120\n",
       "54            -0.2707      0.188     -1.436      0.151        -0.640     0.099\n",
       "55             0.1233      0.057      2.163      0.031         0.011     0.235\n",
       "56            -0.1620      0.117     -1.390      0.165        -0.391     0.067\n",
       "57            -0.0315      0.051     -0.616      0.538        -0.132     0.069\n",
       "58             0.1263      0.079      1.595      0.111        -0.029     0.282\n",
       "59            -0.1238      0.089     -1.387      0.166        -0.299     0.051\n",
       "60             0.0650      0.071      0.915      0.360        -0.074     0.204\n",
       "61             0.0468      0.156      0.300      0.764        -0.259     0.353\n",
       "62            -0.2570      0.275     -0.936      0.349        -0.796     0.282\n",
       "63             0.6461      0.335      1.926      0.054        -0.012     1.304\n",
       "64            -0.4027      0.276     -1.459      0.145        -0.944     0.139\n",
       "65            -0.0497      0.078     -0.634      0.526        -0.204     0.104\n",
       "66            -0.1776      0.084     -2.114      0.035        -0.342    -0.013\n",
       "67            -0.2225      0.280     -0.794      0.427        -0.772     0.327\n",
       "68            -0.0577      0.288     -0.201      0.841        -0.622     0.507\n",
       "69             0.4222      0.307      1.376      0.169        -0.180     1.024\n",
       "70             0.0760      0.206      0.369      0.712        -0.328     0.480\n",
       "71            -0.1405      0.098     -1.431      0.153        -0.333     0.052\n",
       "72            -0.5182      0.442     -1.172      0.241        -1.385     0.349\n",
       "73             0.1958      0.089      2.211      0.027         0.022     0.370\n",
       "74             0.2134      0.068      3.146      0.002         0.080     0.346\n",
       "75             0.0431      0.024      1.815      0.070        -0.003     0.090\n",
       "76             0.1912      0.086      2.213      0.027         0.022     0.361\n",
       "77            -0.0209      0.037     -0.572      0.568        -0.093     0.051\n",
       "78             0.4918      0.462      1.065      0.287        -0.414     1.398\n",
       "79             0.0811      0.027      3.034      0.002         0.029     0.134\n",
       "80            -0.0553      0.030     -1.866      0.062        -0.113     0.003\n",
       "81            -0.0037      0.035     -0.106      0.916        -0.073     0.066\n",
       "82            -0.0177      0.043     -0.412      0.680        -0.102     0.067\n",
       "83            -0.0441      0.025     -1.735      0.083        -0.094     0.006\n",
       "84            -0.4555      0.258     -1.768      0.077        -0.961     0.050\n",
       "85             0.5191      0.393      1.319      0.187        -0.253     1.291\n",
       "86            -0.1478      0.205     -0.719      0.472        -0.551     0.255\n",
       "87            -0.2772      0.084     -3.298      0.001        -0.442    -0.112\n",
       "88             0.0832      0.193      0.431      0.667        -0.296     0.462\n",
       "89            -0.0710      0.109     -0.649      0.516        -0.286     0.144\n",
       "90             0.3356      0.152      2.206      0.028         0.037     0.634\n",
       "91             0.0778      0.041      1.899      0.058        -0.003     0.158\n",
       "92            -0.0662      0.042     -1.592      0.112        -0.148     0.015\n",
       "93            -0.1115      0.030     -3.759      0.000        -0.170    -0.053\n",
       "94             0.0898      0.092      0.980      0.327        -0.090     0.269\n",
       "95             0.1322      0.059      2.251      0.025         0.017     0.247\n",
       "96             0.0661      0.109      0.604      0.546        -0.149     0.281\n",
       "97             0.0460      0.052      0.883      0.377        -0.056     0.148\n",
       "98            -0.0559      0.072     -0.781      0.435        -0.196     0.084\n",
       "99             0.0180      0.046      0.391      0.696        -0.072     0.108\n",
       "100           -0.0014      0.052     -0.026      0.979        -0.104     0.101\n",
       "101            0.1096      0.405      0.271      0.787        -0.685     0.904\n",
       "102          -19.2666     14.465     -1.332      0.183       -47.645     9.112\n",
       "103            0.0777      0.099      0.783      0.434        -0.117     0.272\n",
       "104            0.5621      0.426      1.318      0.188        -0.275     1.399\n",
       "105           -0.1884      0.201     -0.937      0.349        -0.583     0.206\n",
       "106            0.0089      0.178      0.050      0.960        -0.341     0.359\n",
       "107            0.1616      0.111      1.458      0.145        -0.056     0.379\n",
       "108           18.8439     14.441      1.305      0.192        -9.486    47.174\n",
       "109           -0.1156      0.053     -2.178      0.030        -0.220    -0.011\n",
       "110           -0.0365      0.094     -0.390      0.696        -0.220     0.147\n",
       "111            0.1482      0.173      0.856      0.392        -0.192     0.488\n",
       "112            0.2604      0.188      1.389      0.165        -0.107     0.628\n",
       "113            0.0682      0.057      1.195      0.232        -0.044     0.180\n",
       "114           -0.2227      0.250     -0.892      0.373        -0.712     0.267\n",
       "115            0.1087      0.257      0.424      0.672        -0.395     0.612\n",
       "116            0.0333      0.047      0.704      0.482        -0.059     0.126\n",
       "117           -0.0177      0.042     -0.416      0.678        -0.101     0.066\n",
       "118            0.0682      0.070      0.971      0.332        -0.070     0.206\n",
       "119            0.0076      0.038      0.201      0.840        -0.066     0.081\n",
       "120           -0.0212      0.029     -0.742      0.458        -0.077     0.035\n",
       "121            0.0038      0.090      0.042      0.967        -0.173     0.180\n",
       "122            0.1911      0.310      0.617      0.537        -0.417     0.799\n",
       "123           -0.0352      0.082     -0.427      0.670        -0.197     0.126\n",
       "124            0.0173      0.023      0.744      0.457        -0.028     0.063\n",
       "125           -0.0863      0.048     -1.797      0.073        -0.180     0.008\n",
       "==============================================================================\n",
       "Omnibus:                      233.220   Durbin-Watson:                   1.934\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              665.128\n",
       "Skew:                           0.860   Prob(JB):                    3.71e-145\n",
       "Kurtosis:                       5.912   Cond. No.                     4.24e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.24e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runs linear regression and returns summary of model\n",
    "\n",
    "def reg_m(y, x):\n",
    "    model = sm.OLS(y, x.astype(float)).fit()\n",
    "    #fits simple ordinary least squares model\n",
    "    predictions = model.predict(x)\n",
    "    #makes predictions for y based on x\n",
    "    return(model.summary())\n",
    "\n",
    "reg_m(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on training set: 0.0156111113203\n",
      "error on testing set: 0.0197444904381\n"
     ]
    }
   ],
   "source": [
    "model_train = sm.OLS(y_train, x_train.astype(float)).fit()\n",
    "#fits simple ordinary least squares model\n",
    "y_pred_train = model_train.predict(x_train)\n",
    "\n",
    "#fits simple ordinary least squares model\n",
    "y_pred_test = model_train.predict(x_test)\n",
    "\n",
    "#compute mean squared error\n",
    "train_error = np.mean((y_train - y_pred_train)**2)\n",
    "test_error = np.mean((y_test - y_pred_test)**2)\n",
    "\n",
    "print ('error on training set:', train_error)\n",
    "print ('error on testing set:', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d63dcd6cd2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mRBF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_restarts_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(1, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "             kernel=1**2 * RBF(length_scale=10), n_restarts_optimizer=9,\n",
       "             normalize_y=False, optimizer='fmin_l_bfgs_b',\n",
       "             random_state=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_pred, sigma_train = gp.predict(x_train, return_std=True)\n",
    "y_test_pred, sigma_test = gp.predict(x_test, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on training set: 1.0315718890505374e-18\n",
      "error on testing set: 0.030241281607047556\n"
     ]
    }
   ],
   "source": [
    "test_error = np.mean((y_test_pred - y_test)**2)\n",
    "train_error = np.mean((y_train_pred - y_train)**2)\n",
    "\n",
    "\n",
    "print ('error on training set:', train_error)\n",
    "print ('error on testing set:', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
