{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RationalQuadratic, DotProduct\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shapely\n",
    "\n",
    "import time\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "# from collections import Counter\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saved constants </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LU = pd.read_csv('boston_site_LU.csv')\n",
    "LU_width = LU.shape[1]\n",
    "col_names = ['Cropland', 'Golf Course',\n",
    "       'Saltwater Sandy Beach', 'cemetary', 'commercial', 'crop_land',\n",
    "       'forest', 'high_density_residential', 'industrial',\n",
    "       'low_density_residential', 'marina', 'medium_density_residential',\n",
    "       'mining', 'open_land', 'recreational', 'transitional', 'transportation',\n",
    "       'urban_public/institutional', 'waste', 'water', 'wetland', 'Sample Collection Start Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_col_names = ['temp_c', 'relative_humidity', 'wind_kph',\n",
    "                     'pressure_mb','visibility_km', 'UV',\n",
    "                     'precip_today_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = pd.read_csv('wunderground_csvs/weather_averages.csv')\n",
    "lat = W['latitude']\n",
    "lon = W['longitude']\n",
    "\n",
    "stations_lat_lon = list(zip(lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pollutants = ['CO', 'SO', 'NO', 'OZO', 'PM']\n",
    "local_path = 'Appended_AQ_Data/'\n",
    "\n",
    "alphas = [0.000888, 0.000000001, 0.000001, 0.00001, 0.000588]\n",
    "\n",
    "x_label = 'Time'\n",
    "y_labels = pollutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate grid vertices/ticks\n",
    "\n",
    "x = [ -71.20197, -70.96679]\n",
    "y = [42.291441, 42.420578]\n",
    "x_cell = 50\n",
    "y_cell = 50\n",
    "\n",
    "x_min = -71.20197\n",
    "x_max = -70.96679\n",
    "y_min = 42.291441\n",
    "y_max = 42.420578\n",
    "\n",
    "# create ticks\n",
    "x_s = np.linspace(x_min, x_max, x_cell + 1)\n",
    "y_s = np.linspace(y_min, y_max, y_cell + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define functions for land use data cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given lat/lon spot\n",
    "# return indicies for lat/lon grid box bounds\n",
    "# where bounds are:\n",
    "# y_s[lat_index - 1], y_s[lat_index]\n",
    "# x_s[lon_index - 1], x_s[lon_index]\n",
    "def find_gridbox(lat, long):\n",
    "    # locate correct grid box bounds\n",
    "    lat_index = 0\n",
    "    lon_index = 0\n",
    "    for i in range(0, 50):\n",
    "        if x_s[i] < long:\n",
    "            lon_index = i\n",
    "        if y_s[i] > lat:\n",
    "            lon_index = i\n",
    "    return (lat_index, lon_index)\n",
    "\n",
    "\n",
    "# function to parse string from \"Grids\" column in boston_preds.csv\n",
    "# returns list of grid vertices as float64 tuples\n",
    "def parse_str(str_edit):\n",
    "    str_edit = str_edit.replace('[', '')\n",
    "    str_edit = str_edit.replace('(', '')\n",
    "    str_edit = str_edit.replace(']', '')\n",
    "    str_edit = str_edit.replace(')', '')\n",
    "    str_edit = str_edit.replace(',', '')\n",
    "    list = str_edit.split(' ')\n",
    "    return [(np.float64(list[0]), np.float64(list[1])), (np.float64(list[2]), np.float64(list[3])),\n",
    "            (np.float64(list[4]), np.float64(list[5])), (np.float64(list[6]), np.float64(list[7]))]\n",
    "\n",
    "\n",
    "# given lat, lon, and string containing cell verticies as tuples\n",
    "# return boolean value of whether or not lat/lon point is in cell\n",
    "\n",
    "def in_cell(lat, lon, cell):\n",
    "    cell_points = parse_str(cell)\n",
    "    lat_bool = lat > cell_points[0][0] and lat < cell_points[2][0]\n",
    "    lon_bool = lon > cell_points[0][1] and lon < cell_points[2][1]\n",
    "    if lat_bool and lon_bool:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adds land use columns to passed in dataframe,\n",
    "# returns edited version of dataframe\n",
    "\n",
    "def add_columns(df):\n",
    "    \n",
    "    # add land use columns\n",
    "    df['Cropland'] = 0\n",
    "    df['Golf Course'] = 0\n",
    "    df['Saltwater Sandy Beach'] = 0\n",
    "    df['cemetary'] = 0\n",
    "    df['commercial'] = 0\n",
    "    df['crop_land'] = 0\n",
    "    df['forest'] = 0\n",
    "    df['high_density_residential'] = 0\n",
    "    df['industrial'] = 0\n",
    "    df['low_density_residential'] = 0\n",
    "    df['marina'] = 0\n",
    "    df['medium_density_residential'] = 0\n",
    "    df['mining'] = 0\n",
    "    df['open_land'] = 0\n",
    "    df['recreational'] = 0\n",
    "    df['transitional'] = 0\n",
    "    df['transportation'] = 0\n",
    "    df['urban_public/institutional'] = 0\n",
    "    df['waste'] = 0\n",
    "    df['water'] = 0\n",
    "    df['wetland'] = 0\n",
    "    \n",
    "    # add weather columns\n",
    "    df['temp_c'] = 0\n",
    "    df['relative_humidity'] = 0\n",
    "    df['wind_kph'] = 0\n",
    "    df['pressure_mb'] = 0\n",
    "    df['visibility_km'] = 0\n",
    "    df['UV'] = 0\n",
    "    df['precip_today_in'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AQ_df is the Boston AQ hourly\n",
    "# LU_df contains the cells and the landuse\n",
    "# AQ_index is the row of the cell that contains the point in the Boston AQ hourly df\n",
    "# LU_index is the row of the cell that contains the point in the cell and landuse df\n",
    "\n",
    "def fill_in_land_use(AQ_df, LU_df, AQ_index, LU_index):\n",
    "    \n",
    "    for col in range(0, LU_width - 2):#df.loc[i, 'forest'] = df.loc[i, 'forest'] + my_dict['forest']\n",
    "        # print (AQ_index, AQ_width + col, LU_index, col + 1), \"\\n\"\n",
    "        # print AQ_df.iloc[AQ_index, col + AQ_width], \"LU:\", LU_df.iloc[LU_index, col+2]\n",
    "        AQ_df.iloc[AQ_index, col + AQ_width] = LU_df.iloc[LU_index, col + 2]\n",
    "    \n",
    "    return AQ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return numerical distance between two lat lon points\n",
    "def get_distance(aq_lat, aq_lon, weather_lat, weather_lon):\n",
    "    dist = math.sqrt((aq_lat - weather_lat)**2 + (aq_lon - weather_lon)**2)\n",
    "    return dist\n",
    "\n",
    "# given lat lon point, return index of row in weather_averages.csv file\n",
    "# that represents the closest sensor\n",
    "def find_sensor(aq_lat, aq_lon):\n",
    "    \n",
    "    num_stations = len(stations_lat_lon)\n",
    "    \n",
    "    small_dist = get_distance(aq_lat, aq_lon, stations_lat_lon[0][0], stations_lat_lon[0][1])\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(1, num_stations):\n",
    "        new_dist = get_distance(aq_lat, aq_lon, stations_lat_lon[i][0], stations_lat_lon[i][1])\n",
    "        if new_dist < small_dist:\n",
    "            small_dist = new_dist\n",
    "            index = i\n",
    "            \n",
    "    return index\n",
    "\n",
    "#AQ_df is the Boston AQ hourly\n",
    "#LU_df contains the cells and the landuse\n",
    "#AQ_index is the row of the cell that contains the point in the Boston AQ hourly df\n",
    "#LU_index is the row of the cell that contains the point in the cell and landuse df\n",
    "\n",
    "def fill_in_weather(aq_df, weather_df, aq_index):\n",
    "    \n",
    "    weather_index = find_sensor(aq_df['Latitude'][aq_index], aq_df['Longitude'][aq_index])\n",
    "    \n",
    "    for col in weather_col_names:\n",
    "        \n",
    "        aq_df.set_value(aq_index, col, weather_df.get_value(weather_index, col))\n",
    "    \n",
    "    return aq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define functions needed for Gaussian process </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Cleans the times to a parsable format for multiple days in a row\"\"\"\n",
    "def clean_time_multiple(DF, col_name):\n",
    "    \n",
    "    times = []\n",
    "    time = list(DF[col_name])\n",
    "    \n",
    "    prev_T = float(time[0][:2])\n",
    "    day = 0\n",
    "    \n",
    "    for i in range(len(time)):\n",
    "        T = float(time[i][:2])\n",
    "        if prev_T > T:\n",
    "            day += 1\n",
    "        times.append(T + day*24)\n",
    "        prev_T = T\n",
    "    \n",
    "    DF[col_name] = times\n",
    "    \n",
    "    return DF\n",
    "\n",
    "\"\"\"Splits dataframe into train and test data\"\"\"\n",
    "def split_train_test(df):\n",
    "    train, test = train_test_split(df, test_size=.30, random_state=0)\n",
    "    x_train = train[weather_col_names]\n",
    "    y_train = train[\"Measure Value\"]\n",
    "    x_test = test[weather_col_names]\n",
    "    y_test = test[\"Measure Value\"]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\"\"\"\n",
    "Creates Gaussian model, computs r squarred, MSE and log liklihood\n",
    "Given x_train, y_train, x_test, and y_test data, and numerical alpha value\n",
    "Prints train and test R^2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def simple_gaussian(x_train, y_train, x_test, y_test, alpha):\n",
    "    \n",
    "    train_size = x_train.shape[0]\n",
    "    test_size = x_test.shape[0]\n",
    "    \n",
    "    kern = RBF(length_scale = 1)\n",
    "    \n",
    "    gp = GaussianProcessRegressor(kernel=kern, alpha=alpha, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=1, normalize_y=False, copy_X_train=False, random_state=None)\n",
    "    gp.fit(x_train.reshape(train_size, 1), y_train.reshape(train_size, 1))\n",
    "\n",
    "    y_train_pred, sigma_train = gp.predict(x_train.reshape(train_size, 1), return_std=True)\n",
    "    y_test_pred, sigma_test = gp.predict(x_test.reshape(test_size, 1), return_std=True)\n",
    "\n",
    "    # get R^2\n",
    "    r2 = gp.score(x_train.reshape(train_size, 1), y_train.reshape(train_size, 1))\n",
    "    r2_t = gp.score(x_test.reshape(test_size, 1), y_test.reshape(test_size, 1))\n",
    "\n",
    "    # get MSE measurements\n",
    "    MSE_test = np.mean((y_test_pred - y_test.reshape(test_size, 1))**2)\n",
    "    MSE_train = np.mean((y_train_pred - y_train.reshape(train_size, 1))**2)\n",
    "\n",
    "    # get log likelihood\n",
    "    t = gp.log_marginal_likelihood()\n",
    "\n",
    "    # calculate AIC\n",
    "    AIC = 2*len(x_test) - 2*np.log(-t)\n",
    "    \n",
    "    # print R^2 values\n",
    "    print('mean squared error of train data with model = ' + str(MSE_train))\n",
    "    print('mean squared error of test data with model = ' + str(MSE_test))\n",
    "    print('Akaike information criterion = ' + str(AIC))\n",
    "    print('log likelihood of model = ' + str(t))\n",
    "    print('training R^2 value = ' + str(r2))\n",
    "    print('testing R^2 value = ' + str(r2_t))\n",
    "    \n",
    "    return y_train_pred, y_test_pred, gp\n",
    "\n",
    "\"\"\"Plots predicted y values for testing set\"\"\"\n",
    "def plot_predictions(x_test, y_test, y_test_pred, x_label, y_label):\n",
    "    plt.figure(figsize=(13,8))\n",
    "    plt.plot(x_test, y_test, '.', color=\"b\",)\n",
    "    plt.scatter(x_test, y_test_pred, color='r')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(y_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Code to combine AQ data file with land use data and implement Gaussian process</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: this cell takes a long time\n",
    "# for each pollutant, prepare a cleaned dataframe with land use\n",
    "# dataframes list ends up with order: 'CO', 'SO', 'NO', 'OZO', 'PM'\n",
    "\n",
    "sites = LU['Site']\n",
    "dataframes = np.zeros(5, dtype=pd.DataFrame)\n",
    "\n",
    "for i in range(0,5):\n",
    "\n",
    "    filename = local_path + 'appended' + pollutants[i] + '.csv'\n",
    "    \n",
    "    AQ = pd.read_csv(filename)\n",
    "    \n",
    "    '''\n",
    "    lat = AQ['Latitude']\n",
    "    lon = AQ['Longitude']\n",
    "    AQ['lat_lon'] = list(zip(lat, lon))\n",
    "    pts = AQ['lat_lon']\n",
    "    '''\n",
    "    \n",
    "    AQ_width = AQ.shape[1]\n",
    "\n",
    "    AQ = add_columns(AQ)\n",
    "\n",
    "    for sensor in range(AQ.shape[0]):\n",
    "        for cell in range(LU.shape[0]):\n",
    "            if in_cell(AQ['Latitude'][sensor], AQ['Longitude'][sensor], LU['Site'][cell]):\n",
    "                # add cell info to appendedAQ.csv file\n",
    "                AQ = fill_in_land_use(AQ, LU, sensor, cell)\n",
    "        # fill in AQ dataframe with weather data from correct row\n",
    "        AQ = fill_in_weather(AQ, W, sensor)        \n",
    "    \n",
    "    dataframes[i] = AQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-54e6031402b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplot_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-ea0e06ee3d87>\u001b[0m in \u001b[0;36msimple_gaussian\u001b[0;34m(x_train, y_train, x_test, y_test, alpha)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fmin_l_bfgs_b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_X_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Annamira/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# for each pollutant, do a gaussian process\n",
    "\n",
    "for i in range(0,5):\n",
    "    \n",
    "    AQ = dataframes[i]\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_train_test(AQ)\n",
    "\n",
    "    y_train_pred, y_test_pred, gp = simple_gaussian(x_train, y_train, x_test, y_test, alphas[i])\n",
    "\n",
    "    plot_predictions(x_test, y_test, y_test_pred, x_label, y_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
