{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements Gaussian on PM, NO, OZO dataframes that contain Landuse and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RationalQuadratic, DotProduct\n",
    "import statsmodels.api as sm\n",
    "sns.set(style=\"ticks\")\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import gpxpy.geo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm = pd.read_csv(\"merged_pmweather.csv\")\n",
    "no2 = pd.read_csv(\"merged_no2weather.csv\")\n",
    "so2 = pd.read_csv(\"merged_so2weather.csv\")\n",
    "bostonpm= pd.read_csv(\"boston_pm_LU_and_weather.csv\")\n",
    "#pm = pm.iloc[:, 3:]\n",
    "no2 = no2.iloc[:, 3:]\n",
    "so2 = so2.iloc[:, 3:]\n",
    "\n",
    "def split_train_test(df, columns):\n",
    "    train, test = train_test_split(df, random_state=0)\n",
    "    x_train = train[columns].values\n",
    "    y_train = train[\"ppm\"].values\n",
    "    x_test = test[columns].values\n",
    "    y_test = test[\"ppm\"].values\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_train_test(so2,['Sample Collection Start Time', 'forest', 'open_land', 'water',\n",
    "       'wetland', 'transitional', 'urban_public_institution', 'commercial',\n",
    "       'transportation', 'crop_land', 'medium_density_residential',\n",
    "       'industrial', 'outdoor_temperature', 'solar_radiation',\n",
    "       'wind_speed_resultant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian function and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Creates Gaussian model, computs r squarred, MSE and log liklihood.\n",
    "Given x_train, y_train, x_test, and y_test data, and numerical alpha value.\n",
    "Prints train and test R^2. \"\"\"\n",
    "\n",
    "def gaussian(x_train, y_train, x_test, y_test, alpha):\n",
    "    \n",
    "    train_size = x_train.shape[0]\n",
    "    test_size = x_test.shape[0]\n",
    "    \n",
    "    kern =(0.004**2 ) *  RBF(length_scale=62)#C()*RBF(length_scale=2.12e+03)#(1.73**2)*RBF(length_scale = 2.18e+03)\n",
    "\n",
    "    gp = GaussianProcessRegressor(kernel=kern, alpha=alpha, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=1, normalize_y=False, copy_X_train=False, random_state=None)\n",
    "    gp.fit(x_train, y_train.reshape(train_size, 1))\n",
    "\n",
    "    y_train_pred, sigma_train = gp.predict(x_train, return_std=True)\n",
    "    y_test_pred, sigma_test = gp.predict(x_test, return_std=True)\n",
    "\n",
    "    # get R^2\n",
    "    r2 = gp.score(x_train, y_train.reshape(train_size, 1))\n",
    "    r2_t = gp.score(x_test, y_test.reshape(test_size, 1))\n",
    "\n",
    "    # get MSE measurements\n",
    "    MSE_test = np.mean((y_test_pred - y_test.reshape(test_size, 1))**2)\n",
    "    MSE_train = np.mean((y_train_pred - y_train.reshape(train_size, 1))**2)\n",
    "\n",
    "    # get log likelihood\n",
    "    #t=gp.log_marginal_likelihood()\n",
    "\n",
    "    # calculate AIC\n",
    "    #AIC = 2*len(x_test) - 2*np.log(-t)\n",
    "    \n",
    "    # print R^2 values\n",
    "    print('mean squared error of train data with model = ' + str(MSE_train))\n",
    "    print('mean squared error of test data with model = ' + str(MSE_test))\n",
    "    #print('Akaike information criterion = ' + str(AIC))\n",
    "    #print('log likelihood of model = ' + str(t))\n",
    "    print (gp.kernel_)\n",
    "    print('training R^2 value = ' + str(r2))\n",
    "    print('testing R^2 value = ' + str(r2_t)), \"\\n \\n\"\n",
    "    \n",
    "    return y_train_pred, y_test_pred, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error of train data with model = 3.16956311598e-05\n",
      "mean squared error of test data with model = 2.47002891589e-05\n",
      "0.004**2 * RBF(length_scale=62)\n",
      "training R^2 value = 0.31358932428\n",
      "testing R^2 value = 0.332189319755\n"
     ]
    }
   ],
   "source": [
    "y_train_pred, y_test_pred, gp = gaussian(x_train, y_train, x_test, y_test, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"final_boston.csv\")\n",
    "boston[\"Sample Collection Start Time\"] = np.repeat(1, len(boston.index))\n",
    "sites = boston[\"Site\"]\n",
    "boston = boston[['Sample Collection Start Time', 'commercial', 'crop_land', 'forest', 'industrial',\n",
    "       'medium_density_residential', 'open_land', 'transitional',\n",
    "       'transportation', 'urban_public_institution', 'water', 'wetland',\n",
    "       'outdoor_temperature', 'wind_speed_resultant', 'solar_radiation']]\n",
    "boston_aic_preds, sigma_preds = gp.predict(boston, return_std=True)\n",
    "boston_aic_preds = preprocessing.minmax_scale(boston_aic_preds)\n",
    "boston_aic_preds = [item*10 for sublist in boston_aic_preds for item in sublist]\n",
    "def parse_str(str_edit):\n",
    "#   print str_edit\n",
    "    str_edit = str_edit.replace('[', '')\n",
    "    str_edit = str_edit.replace('(', '')\n",
    "    str_edit = str_edit.replace(']', '')\n",
    "    str_edit = str_edit.replace(')', '')\n",
    "    str_edit = str_edit.replace(',', '')\n",
    "    list = str_edit.split(' ')\n",
    "#   print list\n",
    "    return [[np.float64(list[0]), np.float64(list[1])], [np.float64(list[2]), np.float64(list[3])],\n",
    "      [np.float64(list[4]), np.float64(list[5])], [np.float64(list[6]), np.float64(list[7])]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = []\n",
    "for i in sites:\n",
    "    loc.append(parse_str(i))\n",
    "lats, longs = [], []\n",
    "for i in np.arange(len(loc)):\n",
    "    lats.append((loc[i][0][1], loc[i][0][0], boston_aic_preds[i]))\n",
    "    #longs.append(i[0][1])\n",
    "w = pd.DataFrame({\"coords\":pd.Series(lats)})\n",
    "\n",
    "#w.to_csv(\"boston_pm_Gauss_preds.csv\", index=False)\n",
    "w.to_json(\"boston_testing.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9637856273032099"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(boston_aic_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'boston_pm_Gauss_preds.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b9d35e65c5ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"boston_pm_Gauss_preds.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m#pd.read_csv(\"boston_pm_Gauss_preds.csv\")[\"0\"][0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Biyonka\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File b'boston_pm_Gauss_preds.csv' does not exist"
     ]
    }
   ],
   "source": [
    "pd.read_csv(\"boston_pm_Gauss_preds.csv\")[\"0\"][0]\n",
    "#pd.read_csv(\"boston_pm_Gauss_preds.csv\")[\"0\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in y_test_pred for item in sublist]\n",
    "x = np.arange(0, len(flat_list), 1)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(x, y_test, \".\", color=\"#fdc086\", alpha=0.9, markersize=12, label='Test Data')\n",
    "plt.plot(x, y_test_pred, color=\"#1f78b4\", linewidth=1.2, label=\"Gaussian Process Function\")\n",
    "plt.axis([0, len(y_test), 0, 0.035])\n",
    "plt.ylabel('Parts per Million (ppm)', fontsize=24)\n",
    "plt.title('Gaussian Process Fit for SO2 Levels', fontsize=20)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    top='off')\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    right='off',# ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "plt.xticks(np.arange(0, 300, 50), (\"Day 0\", \"Day 1\", \"Day 2\", \"Day 3\", \"Day 4\", \"Day 5\"), fontsize=20)\n",
    "plt.yticks(np.arange(0, 0.035, 0.005), np.arange(0, 0.035, 0.005), fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in y_test_pred for item in sublist]\n",
    "x = np.arange(0, len(flat_list), 1)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(x, y_test, \".\", color=\"#fdc086\", alpha=0.9, markersize=12, label='Test Data')\n",
    "plt.plot(x, y_test_pred, color=\"#1f78b4\", linewidth=1.2, label=\"Gaussian Process Function\")\n",
    "plt.axis([0, len(y_test), 0, 0.25])\n",
    "plt.ylabel('Parts per Million (ppm)', fontsize=24)\n",
    "plt.title('Gaussian Process Fit for PM2.5 Levels', fontsize=20)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    top='off')\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    right='off',# ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "plt.xticks(np.arange(0, 700, 100), (\"Day 0\", \"Day 1\", \"Day 2\", \"Day 3\", \"Day 4\", \"Day 5\", \"Day 6\"), fontsize=20)\n",
    "plt.yticks(np.arange(0, 0.20, 0.04), np.arange(0, 0.20, 0.01), fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in y_test_pred for item in sublist]\n",
    "x = np.arange(0, len(flat_list), 1)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(x, y_test, \".\", color=\"#fdc086\", alpha=0.9, markersize=12, label='Test Data')\n",
    "plt.plot(x, y_test_pred, color=\"#1f78b4\", linewidth=1.2, label=\"Gaussian Process Function\")\n",
    "plt.axis([0, len(y_test), 0, 0.06])\n",
    "plt.ylabel('Parts per Million (ppm)', fontsize=24)\n",
    "plt.title('Gaussian Process Fit for NO2 Levels', fontsize=20)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    top='off')\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected,      # ticks along the bottom edge are off\n",
    "    right='off',# ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "plt.xticks(np.arange(0, 351, 50), (\"Day 0\", \"Day 1\", \"Day 2\", \"Day 3\", \"Day 4\", \"Day 5\", \"Day 6\", \"Day 7\"), fontsize=20)\n",
    "plt.yticks(np.arange(0, 0.06, 0.01), np.arange(0, 0.06, 0.01), fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Gaussian on Boston PM\n",
    "#### Pickles Boston PM data frame with land use and weather columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_label = 'Time'\n",
    "y_label = 'Pollutant'\n",
    "\n",
    "alpha = 0.0608\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_train_test(aq_df2, all_columns)\n",
    "y_train_pred, y_test_pred, gp = gaussian(x_train, y_train, x_test, y_test, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/appendedNO.csv\")\n",
    "\n",
    "no_lat = no_df[\"Latitude\"]\n",
    "no_lon = no_df[\"Longitude\"]\n",
    "\n",
    "no_df['lat_lon'] = list(zip(no_lat, no_lon))\n",
    "no_lat_lon_pts = no_df['lat_lon']\n",
    "del no_df['lat_lon']\n",
    "\n",
    "NO_width = no_df.shape[1]\n",
    "LU_width = df_w_cells.shape[1]\n",
    "no_df = add_columns(no_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Don't need to run again, as I already wrote it to a csv later on\n",
    "no_df = update_df_w_landuse(no_lat_lon_pts, cells, no_df, df_w_cells, col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Don't need to run again\n",
    "no_df = update_df_w_weather(no_lat_lon_pts, no_df, df_w_weather, weather_col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_df.to_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_NO_LU_and_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_NO_LU_and_weather.csv\")\n",
    "no_df2 = pickle_df(no_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.00007\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_train_test(no_df2, all_columns)\n",
    "y_train_pred, y_test_pred, gp = gaussian(x_train, y_train, x_test, y_test, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OZO Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ozo_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/appendedOZO.csv\")\n",
    "\n",
    "ozo_lat = ozo_df[\"Latitude\"]\n",
    "ozo_lon = ozo_df[\"Longitude\"]\n",
    "\n",
    "ozo_df['lat_lon'] = list(zip(ozo_lat, ozo_lon))\n",
    "ozo_lat_lon_pts = ozo_df['lat_lon']\n",
    "del ozo_df['lat_lon']\n",
    "\n",
    "OZO_width = ozo_df.shape[1]\n",
    "LU_width = df_w_cells.shape[1]\n",
    "ozo_df = add_columns(ozo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ozo_df = update_df_w_landuse(ozo_lat_lon_pts, cells, ozo_df, df_w_cells, col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ozo_df = update_df_w_weather(ozo_lat_lon_pts, ozo_df, df_w_weather, weather_col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ozo_df.to_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_ozo_LU_and_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ozo_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_ozo_LU_and_weather.csv\")\n",
    "ozo_df2 = pickle_df(ozo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.00009\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_train_test(ozo_df2, all_columns)\n",
    "y_train_pred, y_test_pred, gp = gaussian(x_train, y_train, x_test, y_test, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/appendedSO.csv\")\n",
    "\n",
    "so_lat = so_df[\"Latitude\"]\n",
    "so_lon = so_df[\"Longitude\"]\n",
    "\n",
    "so_df['lat_lon'] = list(zip(so_lat, so_lon))\n",
    "so_lat_lon_pts = so_df['lat_lon']\n",
    "del so_df['lat_lon']\n",
    "\n",
    "SO_width = so_df.shape[1]\n",
    "LU_width = df_w_cells.shape[1]\n",
    "so_df = add_columns(so_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so_df = update_df_w_landuse(so_lat_lon_pts, cells, so_df, df_w_cells, col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so_df = update_df_w_weather(so_lat_lon_pts, so_df, df_w_weather, weather_col_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so_df.to_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_so_LU_and_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so_df = pd.read_csv(\"C:/Users/Anthony DePinho/Documents/REU 2017/TRiCAM_BostonAQ/Gaussian_Data/boston_so_LU_and_weather.csv\")\n",
    "so_df2 = pickle_df(so_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_train_test(so_df2, all_columns)\n",
    "y_train_pred, y_test_pred, gp = gaussian(x_train, y_train, x_test, y_test, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
